{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reiforcment learning UR-3\n",
    "Setup step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_agents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.trajectories import time_step\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "\n",
    "from tf_env.UR_ENV import UR_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "n_inputs = 6\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(36, activation=\"elu\", input_shape=[n_inputs]),\n",
    "    tf.keras.layers.Dense(12, activation=\"relu\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_iteration=1000\n",
    "# environment = UR_env()\n",
    "\n",
    "# observation = environment.reset()\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "# loss_fn = tf.keras.losses.MSE\n",
    "\n",
    "# for iteration in range(size_iteration):\n",
    "    \n",
    "    \n",
    "#     # if angle < 0, we want proba(left) = 1., or else proba(left) = 0.\n",
    "#     target_probas = np.array([([1.] if obs[2] < 0 else [0.])\n",
    "#                               for obs in observations])\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         left_probas = model(np.array(observations))\n",
    "#         loss = tf.reduce_mean(loss_fn(target_probas, left_probas))\n",
    "#     print(\"\\rIteration: {}, Loss: {:.3f}\".format(iteration, loss.numpy()), end=\"\")\n",
    "#     grads = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "#     actions = (np.random.rand(n_environments, 1) > left_probas.numpy()).astype(np.int32)\n",
    "#     for env_index, env in enumerate(envs):\n",
    "#         obs, reward, done, info = env.step(actions[env_index][0])\n",
    "#         observations[env_index] = obs if not done else env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REUNFORCE algorithms\n",
    "REINFORCE is Reward Incremenent = nonnegative Factor * Offset Reinforcment * Characteristic Eligibility.\n",
    "Based on PG strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_iterations = 100 # @param {type:\"integer\"}\n",
    "fc_layer_params=(50,50,50)\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "number_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "collect_max_episodes = 10 # @param {type:\"integer\"}\n",
    "collect_max_steps = 20 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 1000 # @param {type:\"integer\"}\n",
    "log_interval = 20 # @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Usually we create two environments: one for training and one for evaluation. Most environments are written in pure python, but they can be easily converted to TensorFlow using the TFPyEnvironment wrapper. The original environment's API uses numpy arrays, the TFPyEnvironment converts these to/from Tensors for you to more easily interact with TensorFlow policies and agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_py= UR_env()\n",
    "eval_env_py=UR_env()\n",
    "\n",
    "train_env_tf=tf_py_environment.TFPyEnvironment(train_env_py)\n",
    "eval_env_tf=tf_py_environment.TFPyEnvironment(eval_env_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.action_spec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "The algorithm that we use to solve an RL problem is represented as an Agent. In addition to the REINFORCE agent, TF-Agents provides standard implementations of a variety of Agents such as DQN, DDPG, TD3, PPO and SAC.\n",
    "\n",
    "To create a REINFORCE Agent, we first need an Actor Network that can learn to predict the action given an observation from the environment.\n",
    "\n",
    "We can easily create an Actor Network using the specs of the observations and actions. We can specify the layers in the network which, in this example, is the fc_layer_params argument set to a tuple of ints representing the sizes of each hidden layer (see the Hyperparameters section above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env_tf.observation_spec(),\n",
    "    train_env_tf.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an optimizer to train the network we just created, and a train_step_counter variable to keep track of how many times the network was updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env_tf.time_step_spec(),\n",
    "    train_env_tf.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "In TF-Agents, policies represent the standard notion of policies in RL: given a time_step produce an action or a distribution over actions. The main method is policy_step = policy.action(time_step) where policy_step is a named tuple PolicyStep(action, state, info). The policy_step.action is the action to be applied to the environment, state represents the state for stateful (RNN) policies and info may contain auxiliary information such as log probabilities of the actions.\n",
    "\n",
    "Agents contain two policies: the main policy that is used for evaluation/deployment (agent.policy) and another policy that is used for data collection (agent.collect_policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes. We can compute the average return metric as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(\n",
    "    environment:tf_py_environment.TFPyEnvironment, \n",
    "    policy:tf_agents.policies.tf_policy.TFPolicy, \n",
    "    num_episodes:int=10\n",
    "    ):\n",
    "  \n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "    counter_step=0\n",
    "    counter_step_max=200\n",
    "    \n",
    "\n",
    "    # while not time_step.is_last() or counter_step < counter_step_max:\n",
    "    while counter_step < counter_step_max:\n",
    "      counter_step+=1\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "Reinforcement learning algorithms use replay buffers to store trajectories of experience when executing a policy in an environment. During training, replay buffers are queried for a subset of the trajectories (either a sequential subset or a sample) to \"replay\" the agent's experience.\n",
    "\n",
    "In TF-Agents we use a Driver (see the Driver tutorial for more details) to collect experience in an environment. To use a Driver, we specify an Observer that is a function for the Driver to execute when it receives a trajectory.\n",
    "\n",
    "TFUniformReplayBuffer is the most commonly used replay buffer in TF-Agents, thus we will use here. In TFUniformReplayBuffer the backing buffer storage is done by tensorflow variables and thus is part of the compute graph.\n",
    "\n",
    "The buffer stores batches of elements and has a maximum capacity max_length elements per batch segment. Thus, the total buffer capacity is batch_size x max_length elements. The elements stored in the buffer must all have a matching data spec. When the replay buffer is used for data collection, the spec is the agent's collect data spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=1,\n",
    "    max_length=1000)\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "# replay_buffer = []\n",
    "# replay_observer = [replay_buffer.append]\n",
    "\n",
    "# collect_steps_per_iteration = 10\n",
    "# collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "#   train_env_tf,\n",
    "#   tf_agent.collect_policy,\n",
    "#   observers=replay_observer,\n",
    "#   num_steps=collect_steps_per_iteration).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As REINFORCE learns from whole episodes, we define a function to collect an episode using the given data collection policy and save the data (observations, actions, rewards etc.) as trajectories in the replay buffer. Here we are using 'PyDriver' to run the experience collecting loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount')\n",
      "tf.Tensor(\n",
      "[[[-233.47815   -65.37784  -216.97325 ]\n",
      "  [ -15.516002  121.334    -110.74701 ]]], shape=(1, 2, 3), dtype=float32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [80], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mprint\u001b[39m(trajectories\u001b[39m.\u001b[39m_fields)\n\u001b[0;32m     33\u001b[0m \u001b[39mprint\u001b[39m(trajectories\u001b[39m.\u001b[39mobservation)\n\u001b[1;32m---> 35\u001b[0m loss \u001b[39m=\u001b[39m tf_agent\u001b[39m.\u001b[39;49mtrain(experience\u001b[39m=\u001b[39;49mtrajectories)\n\u001b[0;32m     36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining loss: \u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mloss\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import PyEnvironment\n",
    "from tf_agents.policies.py_policy import PyPolicy\n",
    "from tf_agents.metrics import py_metrics\n",
    "\n",
    "# def collect_episode(environment:PyEnvironment, policy,max_steps:int=1000, max_episodes:int=1):\n",
    "#   driver = py_driver.PyDriver(\n",
    "#     environment,\n",
    "#     py_tf_eager_policy.PyTFEagerPolicy(\n",
    "#       policy, use_tf_function=True),\n",
    "#     replay_observer,\n",
    "#     max_steps=max_steps,\n",
    "#     max_episodes=max_episodes)\n",
    "#   initial_time_step = environment.reset()\n",
    "#   driver.run(initial_time_step)\n",
    "\n",
    "def collect_episode(environment, agent,max_steps:int=100, max_episodes:int=1):\n",
    "  dynamic_step_driver.DynamicStepDriver(\n",
    "    environment,\n",
    "    agent.collect_policy,\n",
    "    observers=replay_observer,\n",
    "    num_steps=10).run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "collect_episode(train_env_tf,tf_agent)\n",
    "dataset = replay_buffer.as_dataset(\n",
    "      sample_batch_size=1)\n",
    "iterator = iter(dataset)\n",
    "loss = None\n",
    "trajectories, _ = next(iterator)\n",
    "print(trajectories._fields)\n",
    "print(trajectories.observation)\n",
    "\n",
    "loss = tf_agent.train(experience=trajectories)\n",
    "print('Training loss: ', loss.loss.numpy())\n",
    "# collect_episode(train_env_py,collect_policy,10,5)\n",
    "# print(replay_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "The training loop involves both collecting data from the environment and optimizing the agent's networks. Along the way, we will occasionally evaluate the agent's policy to see how we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.4047084]\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env_tf, tf_agent.policy, number_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All of the Tensors in `value` must have two outer dimensions. Specifically, tensors must have shape `[B, T] + spec.shape`.\nFull shapes of value tensors:\n  Trajectory(\n{'action': TensorShape([6]),\n 'discount': TensorShape([]),\n 'next_step_type': TensorShape([]),\n 'observation': TensorShape([2, 3]),\n 'policy_info': (),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])}).\nExpected shapes (excluding the two outer dimensions):\n  Trajectory(\n{'action': TensorShape([6]),\n 'discount': TensorShape([]),\n 'next_step_type': TensorShape([]),\n 'observation': TensorShape([2, 3]),\n 'policy_info': (),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])}).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [51], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# print(replay_buffer.as_dataset(sample_batch_size=5))\u001b[39;00m\n\u001b[0;32m     11\u001b[0m trajectories \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m---> 13\u001b[0m train_loss \u001b[39m=\u001b[39m tf_agent\u001b[39m.\u001b[39;49mtrain(experience\u001b[39m=\u001b[39;49mtrajectories)  \n\u001b[0;32m     15\u001b[0m replay_buffer\u001b[39m.\u001b[39mclear()\n\u001b[0;32m     17\u001b[0m step \u001b[39m=\u001b[39m tf_agent\u001b[39m.\u001b[39mtrain_step_counter\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\agents\\tf_agent.py:330\u001b[0m, in \u001b[0;36mTFAgent.train\u001b[1;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    326\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mCannot find _train_fn.  Did \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.__init__ call super?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m       \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_functions:\n\u001b[1;32m--> 330\u001b[0m   loss_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_fn(\n\u001b[0;32m    331\u001b[0m       experience\u001b[39m=\u001b[39mexperience, weights\u001b[39m=\u001b[39mweights, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    332\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m   loss_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train(experience\u001b[39m=\u001b[39mexperience, weights\u001b[39m=\u001b[39mweights, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\utils\\common.py:188\u001b[0m, in \u001b[0;36mfunction_in_tf1.<locals>.maybe_wrap.<locals>.with_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m check_tf1_allowed()\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m has_eager_been_enabled():\n\u001b[0;32m    186\u001b[0m   \u001b[39m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[39;00m\n\u001b[0;32m    187\u001b[0m   \u001b[39m# autodep-like behavior is already expected of fn.\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m resource_variables_enabled():\n\u001b[0;32m    190\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(MISSING_RESOURCE_VARIABLES_ERROR)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\agents\\reinforce\\reinforce_agent.py:247\u001b[0m, in \u001b[0;36mReinforceAgent._train\u001b[1;34m(self, experience, weights)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, experience, weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 247\u001b[0m   experience \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_as_trajectory(experience)\n\u001b[0;32m    249\u001b[0m   \u001b[39m# Add a mask to ensure we reset the return calculation at episode\u001b[39;00m\n\u001b[0;32m    250\u001b[0m   \u001b[39m# boundaries. This is needed in cases where episodes are truncated before\u001b[39;00m\n\u001b[0;32m    251\u001b[0m   \u001b[39m# reaching a terminal state. Note experience is a batch of trajectories\u001b[39;00m\n\u001b[0;32m    252\u001b[0m   \u001b[39m# where reward=next_step.reward so the mask may look shifted at first.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m   non_last_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\n\u001b[0;32m    254\u001b[0m       tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mnot_equal(experience\u001b[39m.\u001b[39mnext_step_type, ts\u001b[39m.\u001b[39mStepType\u001b[39m.\u001b[39mLAST),\n\u001b[0;32m    255\u001b[0m       tf\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\agents\\data_converter.py:340\u001b[0m, in \u001b[0;36mAsTrajectory.__call__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mInput type not supported: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(value))\n\u001b[1;32m--> 340\u001b[0m _validate_trajectory(\n\u001b[0;32m    341\u001b[0m     value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_context\u001b[39m.\u001b[39;49mtrajectory_spec,\n\u001b[0;32m    342\u001b[0m     sequence_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sequence_length,\n\u001b[0;32m    343\u001b[0m     num_outer_dims\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outer_dims)\n\u001b[0;32m    344\u001b[0m value \u001b[39m=\u001b[39m nest_utils\u001b[39m.\u001b[39mprune_extra_keys(\n\u001b[0;32m    345\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_context\u001b[39m.\u001b[39mtrajectory_spec, value)\n\u001b[0;32m    346\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\agents\\data_converter.py:185\u001b[0m, in \u001b[0;36m_validate_trajectory\u001b[1;34m(value, trajectory_spec, sequence_length, num_outer_dims)\u001b[0m\n\u001b[0;32m    181\u001b[0m   shape_str \u001b[39m=\u001b[39m (\n\u001b[0;32m    182\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mtwo outer dimensions\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m num_outer_dims \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m    183\u001b[0m       \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mone outer dimension\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    184\u001b[0m   shape_prefix_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m[B, T]\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m num_outer_dims \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m[B]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 185\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    186\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mAll of the Tensors in `value` must have \u001b[39m\u001b[39m{shape_str}\u001b[39;00m\u001b[39m. Specifically, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    187\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mtensors must have shape `\u001b[39m\u001b[39m{shape_prefix_str}\u001b[39;00m\u001b[39m + spec.shape`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    188\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mFull shapes of value tensors:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m{debug_str_1}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    189\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mExpected shapes (excluding the \u001b[39m\u001b[39m{shape_str}\u001b[39;00m\u001b[39m):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m  \u001b[39m\u001b[39m{debug_str_2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    190\u001b[0m       \u001b[39m.\u001b[39mformat(\n\u001b[0;32m    191\u001b[0m           shape_str\u001b[39m=\u001b[39mshape_str,\n\u001b[0;32m    192\u001b[0m           debug_str_1\u001b[39m=\u001b[39mdebug_str_1,\n\u001b[0;32m    193\u001b[0m           debug_str_2\u001b[39m=\u001b[39mdebug_str_2,\n\u001b[0;32m    194\u001b[0m           shape_prefix_str\u001b[39m=\u001b[39mshape_prefix_str))\n\u001b[0;32m    196\u001b[0m \u001b[39m# If we have a time dimension and a train_sequence_length, make sure they\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m# match.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m sequence_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: All of the Tensors in `value` must have two outer dimensions. Specifically, tensors must have shape `[B, T] + spec.shape`.\nFull shapes of value tensors:\n  Trajectory(\n{'action': TensorShape([6]),\n 'discount': TensorShape([]),\n 'next_step_type': TensorShape([]),\n 'observation': TensorShape([2, 3]),\n 'policy_info': (),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])}).\nExpected shapes (excluding the two outer dimensions):\n  Trajectory(\n{'action': TensorShape([6]),\n 'discount': TensorShape([]),\n 'next_step_type': TensorShape([]),\n 'observation': TensorShape([2, 3]),\n 'policy_info': (),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])})."
     ]
    }
   ],
   "source": [
    "for _ in range(number_iterations):\n",
    "\n",
    "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "  collect_episode(train_env_py, collect_policy,collect_max_steps, collect_max_episodes)\n",
    "\n",
    "  # Use data from the buffer and update the agent's network.\n",
    "  # iterator = iter(replay_buffer.as_dataset(sample_batch_size=1))\n",
    "  iterator = iter(replay_buffer)\n",
    "\n",
    "  # print(replay_buffer.as_dataset(sample_batch_size=5))\n",
    "  trajectories = next(iterator)\n",
    "\n",
    "  train_loss = tf_agent.train(experience=trajectories)  \n",
    "\n",
    "  replay_buffer.clear()\n",
    "\n",
    "  step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env_tf, tf_agent.policy, number_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, number_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
