{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_agents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_env.UR_ENV import UR_env\n",
    "\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.replay_buffers import reverb_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ff\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "# initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
    "collect_steps_per_run = 25 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "# batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "eval_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "# policy_save_interval = 5000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\work\\ENV\\tf_env\\UR_ENV.py:153: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  j_orient =orientation.as_euler('ZXZ',degrees=True)\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "train_env_py= UR_env()\n",
    "eval_env_py=UR_env()\n",
    "\n",
    "train_env_tf=tf_py_environment.TFPyEnvironment(train_env_py)\n",
    "eval_env_tf=tf_py_environment.TFPyEnvironment(eval_env_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information the environment provides as an observation which the policy will use to generate actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))\n",
      "Action Spec:\n",
      "BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(train_env_tf.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(train_env_tf.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import utils\n",
    "\n",
    "utils.validate_py_environment(train_env_py,episodes=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "use_gpu = False\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "To create an SAC Agent, we first need to create the networks that it will train. SAC is an actor-critic agent, so we will need two networks.\n",
    "\n",
    "The critic will give us value estimates for Q(s,a). That is, it will recieve as input an observation and an action, and it will give us an estimate of how good that action was for the given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(train_env_py))\n",
    "\n",
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        observation_fc_layer_params=None,\n",
    "        action_fc_layer_params=None,\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "      observation_spec,\n",
    "      action_spec,\n",
    "      fc_layer_params=actor_fc_layer_params,\n",
    "      continuous_projection_net=(\n",
    "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  train_step = train_utils.create_train_step()\n",
    "\n",
    "  tf_agent = sac_agent.SacAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=alpha_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        train_step_counter=train_step)\n",
    "\n",
    "  tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=train_env_tf.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "# replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size = collect_steps_per_run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  train_env_py.time_step_spec(), train_env_py.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\work\\ENV\\tf_env\\UR_ENV.py:153: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  j_orient =orientation.as_euler('ZXZ',degrees=True)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ResourceScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [2,3], indices.shape [1], params.shape [10000,2,3] [Op:ResourceScatterUpdate]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [18], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m initial_collect_actor \u001b[39m=\u001b[39m actor\u001b[39m.\u001b[39mActor(\n\u001b[0;32m      2\u001b[0m   train_env_py,\n\u001b[0;32m      3\u001b[0m   random_policy,\n\u001b[0;32m      4\u001b[0m   train_step,\n\u001b[0;32m      5\u001b[0m   steps_per_run\u001b[39m=\u001b[39mcollect_steps_per_run,\n\u001b[0;32m      6\u001b[0m   observers\u001b[39m=\u001b[39m[replay_buffer\u001b[39m.\u001b[39madd_batch])\n\u001b[1;32m----> 7\u001b[0m initial_collect_actor\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\train\\actor.py:152\u001b[0m, in \u001b[0;36mActor.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 152\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_step, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_driver\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m    153\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_time_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_state)\n\u001b[0;32m    155\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_summaries \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_interval \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    156\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_summary \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_interval):\n\u001b[0;32m    157\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_metric_summaries()\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\drivers\\py_driver.py:126\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    124\u001b[0m   observer((time_step, action_step_with_previous_state, next_time_step))\n\u001b[0;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m observer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservers:\n\u001b[1;32m--> 126\u001b[0m   observer(traj)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_end_episode_on_boundary:\n\u001b[0;32m    129\u001b[0m   num_episodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(traj\u001b[39m.\u001b[39mis_boundary())\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\replay_buffer.py:83\u001b[0m, in \u001b[0;36mReplayBuffer.add_batch\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_batch\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[0;32m     73\u001b[0m   \u001b[39m\"\"\"Adds a batch of items to the replay buffer.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m    Adds `items` to the replay buffer.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_batch(items)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:205\u001b[0m, in \u001b[0;36mTFUniformReplayBuffer._add_batch\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    203\u001b[0m write_rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_rows_for_id(id_)\n\u001b[0;32m    204\u001b[0m write_id_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id_table\u001b[39m.\u001b[39mwrite(write_rows, id_)\n\u001b[1;32m--> 205\u001b[0m write_data_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_table\u001b[39m.\u001b[39;49mwrite(write_rows, items)\n\u001b[0;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mgroup(write_id_op, write_data_op)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\table.py:128\u001b[0m, in \u001b[0;36mTable.write\u001b[1;34m(self, rows, values, slots)\u001b[0m\n\u001b[0;32m    126\u001b[0m flattened_slots \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(slots)\n\u001b[0;32m    127\u001b[0m flattened_values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(values)\n\u001b[1;32m--> 128\u001b[0m write_ops \u001b[39m=\u001b[39m [\n\u001b[0;32m    129\u001b[0m     tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mscatter_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slot2storage_map[slot], rows,\n\u001b[0;32m    130\u001b[0m                                 value)\u001b[39m.\u001b[39mop\n\u001b[0;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m (slot, value) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flattened_slots, flattened_values)\n\u001b[0;32m    132\u001b[0m ]\n\u001b[0;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mgroup(\u001b[39m*\u001b[39mwrite_ops)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\table.py:129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    126\u001b[0m flattened_slots \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(slots)\n\u001b[0;32m    127\u001b[0m flattened_values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(values)\n\u001b[0;32m    128\u001b[0m write_ops \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 129\u001b[0m     tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mscatter_update(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot2storage_map[slot], rows,\n\u001b[0;32m    130\u001b[0m                                 value)\u001b[39m.\u001b[39mop\n\u001b[0;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m (slot, value) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flattened_slots, flattened_values)\n\u001b[0;32m    132\u001b[0m ]\n\u001b[0;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mgroup(\u001b[39m*\u001b[39mwrite_ops)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py:431\u001b[0m, in \u001b[0;36mscatter_update\u001b[1;34m(ref, indices, updates, use_locking, name)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[39mif\u001b[39;00m ref\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m_is_ref_dtype:\n\u001b[0;32m    429\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_state_ops\u001b[39m.\u001b[39mscatter_update(ref, indices, updates,\n\u001b[0;32m    430\u001b[0m                                       use_locking\u001b[39m=\u001b[39muse_locking, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m--> 431\u001b[0m \u001b[39mreturn\u001b[39;00m ref\u001b[39m.\u001b[39m_lazy_read(gen_resource_variable_ops\u001b[39m.\u001b[39;49mresource_scatter_update(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    432\u001b[0m     ref\u001b[39m.\u001b[39;49mhandle, indices, ops\u001b[39m.\u001b[39;49mconvert_to_tensor(updates, ref\u001b[39m.\u001b[39;49mdtype),\n\u001b[0;32m    433\u001b[0m     name\u001b[39m=\u001b[39;49mname))\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:1174\u001b[0m, in \u001b[0;36mresource_scatter_update\u001b[1;34m(resource, indices, updates, name)\u001b[0m\n\u001b[0;32m   1172\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1173\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1174\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   1175\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   1176\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ResourceScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [2,3], indices.shape [1], params.shape [10000,2,3] [Op:ResourceScatterUpdate]"
     ]
    }
   ],
   "source": [
    "initial_collect_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=collect_steps_per_run,\n",
    "  observers=[replay_buffer.add_batch])\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import py_metrics\n",
    "# from tf_agents.train import learner\n",
    "# import os\n",
    "# import tempfile\n",
    "\n",
    "# tempdir = tempfile.gettempdir()\n",
    "\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  collect_policy,\n",
    "  train_step,\n",
    "  steps_per_run=1,\n",
    "  metrics=actor.collect_metrics(10),\n",
    "  observers=[replay_buffer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fea1847bf10>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fea1847bf10>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
      "2022-10-28 13:16:04.633751: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train import triggers\n",
    "\n",
    "policy_save_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers,\n",
    "  strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/UR_Reinforsment_Learning/tf_env/UR_ENV.py:121: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  orientation=orientation.as_euler('ZXZ',degrees=True)\n"
     ]
    }
   ],
   "source": [
    "def get_eval_metrics():\n",
    "  eval_actor.run()\n",
    "  results = {}\n",
    "  for metric in eval_actor.metrics:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "metrics = get_eval_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: AverageReturn = 493.899994, AverageEpisodeLength = 1001.000000\n"
     ]
    }
   ],
   "source": [
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'CheckNumerics' defined at (most recent call last):\n    File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n      self.run()\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/tf_agent.py\", line 330, in train\n      loss_info = self._train_fn(\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/utils/common.py\", line 188, in with_check_resource_vars\n      return fn(*fn_args, **fn_kwargs)\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/sac/sac_agent.py\", line 323, in _train\n      tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\nNode: 'CheckNumerics'\nCritic loss is inf or nan. : Tensor had Inf values\n\t [[{{node CheckNumerics}}]] [Op:__inference__train_210142]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m      2\u001b[0m   \u001b[39m# Training.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   collect_actor\u001b[39m.\u001b[39mrun()\n\u001b[0;32m----> 4\u001b[0m   loss_info \u001b[39m=\u001b[39m agent_learner\u001b[39m.\u001b[39;49mrun(iterations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m   \u001b[39m# Evaluating.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m   step \u001b[39m=\u001b[39m agent_learner\u001b[39m.\u001b[39mtrain_step_numpy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/train/learner.py:283\u001b[0m, in \u001b[0;36mLearner.run\u001b[0;34m(self, iterations, iterator, parallel_iterations)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_summary_writer\u001b[39m.\u001b[39mas_default(), \\\n\u001b[1;32m    279\u001b[0m      common\u001b[39m.\u001b[39msoft_device_placement(), \\\n\u001b[1;32m    280\u001b[0m      tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv2\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mrecord_if(_summary_record_if), \\\n\u001b[1;32m    281\u001b[0m      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m    282\u001b[0m   iterator \u001b[39m=\u001b[39m iterator \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experience_iterator\n\u001b[0;32m--> 283\u001b[0m   loss_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(tf\u001b[39m.\u001b[39;49mconstant(iterations),\n\u001b[1;32m    284\u001b[0m                           iterator,\n\u001b[1;32m    285\u001b[0m                           parallel_iterations)\n\u001b[1;32m    287\u001b[0m   train_step_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    288\u001b[0m   \u001b[39mfor\u001b[39;00m trigger \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtriggers:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'CheckNumerics' defined at (most recent call last):\n    File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n      self.run()\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/tf_agent.py\", line 330, in train\n      loss_info = self._train_fn(\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/utils/common.py\", line 188, in with_check_resource_vars\n      return fn(*fn_args, **fn_kwargs)\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/sac/sac_agent.py\", line 323, in _train\n      tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\nNode: 'CheckNumerics'\nCritic loss is inf or nan. : Tensor had Inf values\n\t [[{{node CheckNumerics}}]] [Op:__inference__train_210142]"
     ]
    }
   ],
   "source": [
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "eval_interval = 1000\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  collect_actor.run()\n",
    "  loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
