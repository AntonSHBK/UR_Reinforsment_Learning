{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_agents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_env.UR_ENV import UR_env\n",
    "\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.replay_buffers import reverb_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ff\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 1000 # @param {type:\"integer\"}\n",
    "\n",
    "# initial_collect_steps = 10000 # @param {type:\"integer\"}\n",
    "collect_steps_per_run = 25 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "# batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "eval_interval = 10000 # @param {type:\"integer\"}\n",
    "\n",
    "# policy_save_interval = 5000 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\work\\ENV\\tf_env\\UR_ENV.py:380: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  j_orient =orientation.as_euler('ZXZ',degrees=True)\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "train_env_py= UR_env()\n",
    "eval_env_py=UR_env()\n",
    "\n",
    "train_env_tf=tf_py_environment.TFPyEnvironment(train_env_py)\n",
    "eval_env_tf=tf_py_environment.TFPyEnvironment(eval_env_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the information the environment provides as an observation which the policy will use to generate actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))\n",
      "Action Spec:\n",
      "BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(train_env_tf.time_step_spec().observation)\n",
    "print('Action Spec:')\n",
    "print(train_env_tf.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import utils\n",
    "\n",
    "utils.validate_py_environment(train_env_py,episodes=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "use_gpu = False\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "To create an SAC Agent, we first need to create the networks that it will train. SAC is an actor-critic agent, so we will need two networks.\n",
    "\n",
    "The critic will give us value estimates for Q(s,a). That is, it will recieve as input an observation and an action, and it will give us an estimate of how good that action was for the given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(train_env_py))\n",
    "\n",
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        observation_fc_layer_params=None,\n",
    "        action_fc_layer_params=None,\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_step_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "      observation_spec,\n",
    "      action_spec,\n",
    "      fc_layer_params=actor_fc_layer_params,\n",
    "      continuous_projection_net=(\n",
    "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  train_step = train_utils.create_train_step()\n",
    "\n",
    "  tf_agent = sac_agent.SacAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=alpha_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        train_step_counter=train_step)\n",
    "\n",
    "  tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=train_env_tf.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "# replay_observer = [replay_buffer.add_batch]\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size = collect_steps_per_run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  train_env_py.time_step_spec(), train_env_py.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_collect_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=collect_steps_per_run,\n",
    "  observers=[replay_buffer.add_batch])\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import py_metrics\n",
    "# from tf_agents.train import learner\n",
    "# import os\n",
    "# import tempfile\n",
    "\n",
    "# tempdir = tempfile.gettempdir()\n",
    "\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  collect_policy,\n",
    "  train_step,\n",
    "  steps_per_run=1,\n",
    "  metrics=actor.collect_metrics(10),\n",
    "  observers=[replay_buffer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fea1847bf10>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fea1847bf10>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
      "2022-10-28 13:16:04.633751: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train import triggers\n",
    "\n",
    "policy_save_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers,\n",
    "  strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/UR_Reinforsment_Learning/tf_env/UR_ENV.py:121: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  orientation=orientation.as_euler('ZXZ',degrees=True)\n"
     ]
    }
   ],
   "source": [
    "def get_eval_metrics():\n",
    "  eval_actor.run()\n",
    "  results = {}\n",
    "  for metric in eval_actor.metrics:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "metrics = get_eval_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: AverageReturn = 493.899994, AverageEpisodeLength = 1001.000000\n"
     ]
    }
   ],
   "source": [
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'CheckNumerics' defined at (most recent call last):\n    File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n      self.run()\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/tf_agent.py\", line 330, in train\n      loss_info = self._train_fn(\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/utils/common.py\", line 188, in with_check_resource_vars\n      return fn(*fn_args, **fn_kwargs)\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/sac/sac_agent.py\", line 323, in _train\n      tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\nNode: 'CheckNumerics'\nCritic loss is inf or nan. : Tensor had Inf values\n\t [[{{node CheckNumerics}}]] [Op:__inference__train_210142]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m      2\u001b[0m   \u001b[39m# Training.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   collect_actor\u001b[39m.\u001b[39mrun()\n\u001b[0;32m----> 4\u001b[0m   loss_info \u001b[39m=\u001b[39m agent_learner\u001b[39m.\u001b[39;49mrun(iterations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m   \u001b[39m# Evaluating.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m   step \u001b[39m=\u001b[39m agent_learner\u001b[39m.\u001b[39mtrain_step_numpy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/train/learner.py:283\u001b[0m, in \u001b[0;36mLearner.run\u001b[0;34m(self, iterations, iterator, parallel_iterations)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_summary_writer\u001b[39m.\u001b[39mas_default(), \\\n\u001b[1;32m    279\u001b[0m      common\u001b[39m.\u001b[39msoft_device_placement(), \\\n\u001b[1;32m    280\u001b[0m      tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv2\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mrecord_if(_summary_record_if), \\\n\u001b[1;32m    281\u001b[0m      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m    282\u001b[0m   iterator \u001b[39m=\u001b[39m iterator \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experience_iterator\n\u001b[0;32m--> 283\u001b[0m   loss_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(tf\u001b[39m.\u001b[39;49mconstant(iterations),\n\u001b[1;32m    284\u001b[0m                           iterator,\n\u001b[1;32m    285\u001b[0m                           parallel_iterations)\n\u001b[1;32m    287\u001b[0m   train_step_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    288\u001b[0m   \u001b[39mfor\u001b[39;00m trigger \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtriggers:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'CheckNumerics' defined at (most recent call last):\n    File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n      self.run()\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/tf_agent.py\", line 330, in train\n      loss_info = self._train_fn(\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/utils/common.py\", line 188, in with_check_resource_vars\n      return fn(*fn_args, **fn_kwargs)\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/sac/sac_agent.py\", line 323, in _train\n      tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\nNode: 'CheckNumerics'\nCritic loss is inf or nan. : Tensor had Inf values\n\t [[{{node CheckNumerics}}]] [Op:__inference__train_210142]"
     ]
    }
   ],
   "source": [
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "eval_interval = 1000\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  collect_actor.run()\n",
    "  loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
