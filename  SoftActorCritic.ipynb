{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_agents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_env.UR_ENV import UR_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "train_env_py= UR_env()\n",
    "eval_env_py=UR_env()\n",
    "\n",
    "# train_env_tf=tf_py_environment.TFPyEnvironment(train_env_py)\n",
    "# eval_env_tf=tf_py_environment.TFPyEnvironment(eval_env_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.environments import utils\n",
    "\n",
    "utils.validate_py_environment(train_env_py,episodes=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "use_gpu = True\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(train_env_py))\n",
    "\n",
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        observation_fc_layer_params=None,\n",
    "        action_fc_layer_params=None,\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "\n",
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "      observation_spec,\n",
    "      action_spec,\n",
    "      fc_layer_params=actor_fc_layer_params,\n",
    "      continuous_projection_net=(\n",
    "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "with strategy.scope():\n",
    "  train_step = train_utils.create_train_step()\n",
    "\n",
    "  tf_agent = sac_agent.SacAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=alpha_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        train_step_counter=train_step)\n",
    "\n",
    "  tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "# replay_buffer_capacity = 100 # @param {type:\"integer\"}\n",
    "# batch_size = 50 # @param {type:\"integer\"}\n",
    "\n",
    "# replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "#     tf_agent.collect_data_spec,\n",
    "#     batch_size=batch_size,\n",
    "#     max_length=replay_buffer_capacity)\n",
    "\n",
    "# replay_observer = [replay_buffer.add_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import reverb\n",
    "\n",
    "rate_limiter=reverb.rate_limiters.SampleToInsertRatio(samples_per_insert=3.0, min_size_to_sample=3, error_buffer=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:162]  Initializing TFRecordCheckpointer in /tmp/tmpb14bs9nm.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:567] Loading latest checkpoint from /tmp/tmpb14bs9nm\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 33063\n"
     ]
    }
   ],
   "source": [
    "replay_buffer_capacity = 10000 # @param {type:\"integer\"}\n",
    "batch_size = 256 # @param {type:\"integer\"}\n",
    "\n",
    "table_name = 'uniform_table'\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1))\n",
    "\n",
    "reverb_server = reverb.Server([table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "\n",
    "reverb_replay = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    sequence_length=2,\n",
    "    table_name=table_name,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "dataset = reverb_replay.as_dataset(\n",
    "      sample_batch_size=batch_size, num_steps=2).prefetch(50)\n",
    "experience_dataset_fn = lambda: dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_py_policy\n",
    "\n",
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  train_env_py.time_step_spec(), train_env_py.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train import actor\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "\n",
    "initial_collect_steps = 100 # @param {type:\"integer\"}\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  reverb_replay.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2,\n",
    "  stride_length=1)\n",
    "\n",
    "initial_collect_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=initial_collect_steps,\n",
    "  observers=[rb_observer])\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.train import learner\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "tempdir = tempfile.gettempdir()\n",
    "\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "collect_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  collect_policy,\n",
    "  train_step,\n",
    "  steps_per_run=1,\n",
    "  metrics=actor.collect_metrics(10),\n",
    "  summary_dir=os.path.join(tempdir, learner.TRAIN_DIR),\n",
    "  observers=[rb_observer, env_step_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 20 # @param {type:\"integer\"}\n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  eval_policy,\n",
    "  train_step,\n",
    "  episodes_per_run=num_eval_episodes,\n",
    "  metrics=actor.eval_metrics(num_eval_episodes),\n",
    "  summary_dir=os.path.join(tempdir, 'eval'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fea1847bf10>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
      "WARNING:absl:WARNING: Could not serialize policy.distribution() for policy \"<tf_agents.policies.actor_policy.ActorPolicy object at 0x7fea1847bf10>\". Calling saved_model.distribution() will raise the following assertion error: SquashToSpecNormal.__init__() got an unexpected keyword argument 'loc'\n",
      "2022-10-28 13:16:04.633751: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train import triggers\n",
    "\n",
    "policy_save_interval = 5000 # @param {type:\"integer\"}\n",
    "\n",
    "saved_model_dir = os.path.join(tempdir, learner.POLICY_SAVED_MODEL_DIR)\n",
    "\n",
    "# Triggers to save the agent's policy checkpoints.\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        saved_model_dir,\n",
    "        tf_agent,\n",
    "        train_step,\n",
    "        interval=policy_save_interval),\n",
    "    triggers.StepPerSecondLogTrigger(train_step, interval=1000),\n",
    "]\n",
    "\n",
    "agent_learner = learner.Learner(\n",
    "  tempdir,\n",
    "  train_step,\n",
    "  tf_agent,\n",
    "  experience_dataset_fn,\n",
    "  triggers=learning_triggers,\n",
    "  strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/UR_Reinforsment_Learning/tf_env/UR_ENV.py:121: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  orientation=orientation.as_euler('ZXZ',degrees=True)\n"
     ]
    }
   ],
   "source": [
    "def get_eval_metrics():\n",
    "  eval_actor.run()\n",
    "  results = {}\n",
    "  for metric in eval_actor.metrics:\n",
    "    results[metric.name] = metric.result()\n",
    "  return results\n",
    "\n",
    "metrics = get_eval_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0: AverageReturn = 493.899994, AverageEpisodeLength = 1001.000000\n"
     ]
    }
   ],
   "source": [
    "def log_eval_metrics(step, metrics):\n",
    "  eval_results = (', ').join(\n",
    "      '{} = {:.6f}'.format(name, result) for name, result in metrics.items())\n",
    "  print('step = {0}: {1}'.format(step, eval_results))\n",
    "\n",
    "log_eval_metrics(0, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = get_eval_metrics()[\"AverageReturn\"]\n",
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (15529) so Table uniform_table is accessed directly without gRPC.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'CheckNumerics' defined at (most recent call last):\n    File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n      self.run()\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/tf_agent.py\", line 330, in train\n      loss_info = self._train_fn(\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/utils/common.py\", line 188, in with_check_resource_vars\n      return fn(*fn_args, **fn_kwargs)\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/sac/sac_agent.py\", line 323, in _train\n      tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\nNode: 'CheckNumerics'\nCritic loss is inf or nan. : Tensor had Inf values\n\t [[{{node CheckNumerics}}]] [Op:__inference__train_210142]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m      2\u001b[0m   \u001b[39m# Training.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m   collect_actor\u001b[39m.\u001b[39mrun()\n\u001b[0;32m----> 4\u001b[0m   loss_info \u001b[39m=\u001b[39m agent_learner\u001b[39m.\u001b[39;49mrun(iterations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m   \u001b[39m# Evaluating.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m   step \u001b[39m=\u001b[39m agent_learner\u001b[39m.\u001b[39mtrain_step_numpy\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tf_agents/train/learner.py:283\u001b[0m, in \u001b[0;36mLearner.run\u001b[0;34m(self, iterations, iterator, parallel_iterations)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_summary_writer\u001b[39m.\u001b[39mas_default(), \\\n\u001b[1;32m    279\u001b[0m      common\u001b[39m.\u001b[39msoft_device_placement(), \\\n\u001b[1;32m    280\u001b[0m      tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv2\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mrecord_if(_summary_record_if), \\\n\u001b[1;32m    281\u001b[0m      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m    282\u001b[0m   iterator \u001b[39m=\u001b[39m iterator \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experience_iterator\n\u001b[0;32m--> 283\u001b[0m   loss_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(tf\u001b[39m.\u001b[39;49mconstant(iterations),\n\u001b[1;32m    284\u001b[0m                           iterator,\n\u001b[1;32m    285\u001b[0m                           parallel_iterations)\n\u001b[1;32m    287\u001b[0m   train_step_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_step\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    288\u001b[0m   \u001b[39mfor\u001b[39;00m trigger \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtriggers:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'CheckNumerics' defined at (most recent call last):\n    File \"/usr/lib/python3.10/threading.py\", line 973, in _bootstrap\n      self._bootstrap_inner()\n    File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n      self.run()\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/tf_agent.py\", line 330, in train\n      loss_info = self._train_fn(\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/utils/common.py\", line 188, in with_check_resource_vars\n      return fn(*fn_args, **fn_kwargs)\n    File \"/home/anton/.local/lib/python3.10/site-packages/tf_agents/agents/sac/sac_agent.py\", line 323, in _train\n      tf.debugging.check_numerics(critic_loss, 'Critic loss is inf or nan.')\nNode: 'CheckNumerics'\nCritic loss is inf or nan. : Tensor had Inf values\n\t [[{{node CheckNumerics}}]] [Op:__inference__train_210142]"
     ]
    }
   ],
   "source": [
    "num_iterations = 10000 # @param {type:\"integer\"}\n",
    "log_interval = 5000 # @param {type:\"integer\"}\n",
    "eval_interval = 1000\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Training.\n",
    "  collect_actor.run()\n",
    "  loss_info = agent_learner.run(iterations=1)\n",
    "\n",
    "  # Evaluating.\n",
    "  step = agent_learner.train_step_numpy\n",
    "\n",
    "  if eval_interval and step % eval_interval == 0:\n",
    "    metrics = get_eval_metrics()\n",
    "    log_eval_metrics(step, metrics)\n",
    "    returns.append(metrics[\"AverageReturn\"])\n",
    "\n",
    "  if log_interval and step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, loss_info.loss.numpy()))\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
