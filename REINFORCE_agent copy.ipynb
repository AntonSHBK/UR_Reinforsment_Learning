{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Agent\n",
    "\n",
    "## Setup step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%udo` not found.\n"
     ]
    }
   ],
   "source": [
    "%udo apt-get update\n",
    "%pip install scipy\n",
    "%pip install tensorflow\n",
    "%pip install tf-agents\n",
    "%pip install dm-reverb[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_agents\n",
    "import pandas\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=18)\n",
    "mpl.rc('ytick', labelsize=18)\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# from tf_agents.environments import py_environment\n",
    "# from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "# from tf_agents.policies import py_tf_eager_policy\n",
    "# from tf_agents.drivers import py_driver\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.environments import utils\n",
    "# from tf_agents.specs import array_spec\n",
    "# from tf_agents.trajectories import time_step\n",
    "# from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.policies import random_tf_policy\n",
    "# from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "# from tf_agents.replay_buffers import reverb_utils\n",
    "\n",
    "from tf_env.UR_ENV import UR_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_iterations = 1000 # @param {type:\"integer\"}\n",
    "discount = 0.99 # @param {type:\"number\"}\n",
    "fc_layer_params=(50,50)\n",
    "learning_rate = 1e-5 # @param {type:\"number\"}\n",
    "number_eval_episodes = 3 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 5 # @param {type:\"integer\"}\n",
    "# collect_max_steps = 20 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 1000 # @param {type:\"integer\"}\n",
    "log_interval = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 20 # @param {type:\"integer\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "Usually we create two environments: one for training and one for evaluation. Most environments are written in pure python, but they can be easily converted to TensorFlow using the TFPyEnvironment wrapper. The original environment's API uses numpy arrays, the TFPyEnvironment converts these to/from Tensors for you to more easily interact with TensorFlow policies and agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_py= UR_env()\n",
    "eval_env_py=UR_env()\n",
    "\n",
    "train_env_tf=tf_py_environment.TFPyEnvironment(train_env_py)\n",
    "eval_env_tf=tf_py_environment.TFPyEnvironment(eval_env_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.discount_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.float32, name='reward')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.reward_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'observation': BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(\n",
       "{'discount': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>,\n",
       " 'observation': <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=\n",
       "array([[[  85.35, -194.25,  608.8 ],\n",
       "        [   0.  ,   90.  ,  -90.  ]]], dtype=float32)>,\n",
       " 'reward': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " 'step_type': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([0])>})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env_tf.current_time_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_env_tf.batch_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pandas.DataFrame(columns=['step_type', 'reward', 'discount', 'observation'])\n",
    "# t_step = train_env_tf.reset()\n",
    "# df = df.append(\n",
    "#     {\n",
    "#     'step_type': t_step.step_type, \n",
    "#     'reward': t_step.reward, \n",
    "#     'discount': t_step.discount, \n",
    "#     'observation': t_step.observation\n",
    "#     }, \n",
    "#     ignore_index=True\n",
    "# )\n",
    "# while not t_step.is_last():\n",
    "#     action = np.array([[1,1,1,1,1,1]],dtype=np.float32)\n",
    "#     t_step = train_env_tf.step(action)\n",
    "#     df = df.append(\n",
    "#         {\n",
    "#         'step_type': t_step.step_type, \n",
    "#         'reward': t_step.reward, \n",
    "#         'discount': t_step.discount, \n",
    "#         'observation': t_step.observation\n",
    "#         }, \n",
    "#         ignore_index=True\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "### Actor Network Policies\n",
    "The algorithm that we use to solve an RL problem is represented as an Agent. In addition to the REINFORCE agent, TF-Agents provides standard implementations of a variety of Agents such as DQN, DDPG, TD3, PPO and SAC.\n",
    "\n",
    "To create a REINFORCE Agent, we first need an Actor Network that can learn to predict the action given an observation from the environment.\n",
    "\n",
    "We can easily create an Actor Network using the specs of the observations and actions. We can specify the layers in the network which, in this example, is the fc_layer_params argument set to a tuple of ints representing the sizes of each hidden layer (see the Hyperparameters section above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network\n",
    "\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_env_tf.observation_spec(),\n",
    "    train_env_tf.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an optimizer to train the network we just created, and a train_step_counter variable to keep track of how many times the network was updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_counter = tf.Variable(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env_tf.time_step_spec(),\n",
    "    train_env_tf.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(6,), dtype=tf.float32, name='action', minimum=array(-1., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(2, 3), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_agent.collect_data_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "In TF-Agents, policies represent the standard notion of policies in RL: given a time_step produce an action or a distribution over actions. The main method is policy_step = policy.action(time_step) where policy_step is a named tuple PolicyStep(action, state, info). The policy_step.action is the action to be applied to the environment, state represents the state for stateful (RNN) policies and info may contain auxiliary information such as log probabilities of the actions.\n",
    "\n",
    "Agents contain two policies: the main policy that is used for evaluation/deployment (agent.policy) and another policy that is used for data collection (agent.collect_policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(\n",
    "    time_step_spec=train_env_tf.time_step_spec(),\n",
    "    action_spec=train_env_tf.action_spec()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Evaluation\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes. We can compute the average return metric as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "Reinforcement learning algorithms use replay buffers to store trajectories of experience when executing a policy in an environment. During training, replay buffers are queried for a subset of the trajectories (either a sequential subset or a sample) to \"replay\" the agent's experience.\n",
    "\n",
    "In TF-Agents we use a Driver (see the Driver tutorial for more details) to collect experience in an environment. To use a Driver, we specify an Observer that is a function for the Driver to execute when it receives a trajectory.\n",
    "\n",
    "TFUniformReplayBuffer is the most commonly used replay buffer in TF-Agents, thus we will use here. In TFUniformReplayBuffer the backing buffer storage is done by tensorflow variables and thus is part of the compute graph.\n",
    "\n",
    "The buffer stores batches of elements and has a maximum capacity max_length elements per batch segment. Thus, the total buffer capacity is batch_size x max_length elements. The elements stored in the buffer must all have a matching data spec. When the replay buffer is used for data collection, the spec is the agent's collect data spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = tf_agent.collect_data_spec,\n",
    "    batch_size = train_env_tf.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "    \n",
    "rb_observer = [replay_buffer.add_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(train_env_tf.batch_size)\n",
    "print(train_env_tf.batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As REINFORCE learns from whole episodes, we define a function to collect an episode using the given data collection policy and save the data (observations, actions, rewards etc.) as trajectories in the replay buffer. Here we are using 'PyDriver' to run the experience collecting loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents import trajectories\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "    ts = environment.current_time_step()\n",
    "    action_step = policy.action(ts)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectories.from_transition(ts, action_step, next_time_step)\n",
    "    buffer.add_batch(traj)\n",
    "    \n",
    "def collect_data(env, policy, buffer, steps):\n",
    "    for _ in range(steps):\n",
    "        collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env_tf, random_policy, replay_buffer, steps=100)\n",
    "\n",
    "# def collect_episode(environment, policy, max_episodes):\n",
    "#   collect_op = dynamic_step_driver.DynamicStepDriver(\n",
    "#     environment,\n",
    "#     policy,\n",
    "#     observers=rb_observer,\n",
    "#     num_steps=max_episodes)\n",
    "\n",
    "#   collect_op.run()\n",
    "\n",
    "# collect_episode(train_env_tf, collect_policy, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "  num_parallel_calls=3, \n",
    "  sample_batch_size=100, \n",
    "  num_steps=2\n",
    ").prefetch(3)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "The training loop involves both collecting data from the environment and optimizing the agent's networks. Along the way, we will occasionally evaluate the agent's policy to see how we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.6137366]\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "tf_agent.train = common.function(tf_agent.train)\n",
    "# Reset the train step\n",
    "tf_agent.train_step_counter.assign(0)\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env_tf, tf_agent.policy, number_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 10: loss = 38.66746520996094\n",
      "step = 20: loss = -0.14187759160995483\n",
      "step = 20: Average Return = -2.6443166732788086\n",
      "step = 30: loss = -0.6900845170021057\n",
      "step = 40: loss = -1.945279836654663\n",
      "step = 40: Average Return = -2.405251979827881\n",
      "step = 50: loss = -0.6587789058685303\n",
      "step = 60: loss = 6.127322673797607\n",
      "step = 60: Average Return = -3.2914066314697266\n",
      "step = 70: loss = -10.325736999511719\n",
      "step = 80: loss = -0.4746857285499573\n",
      "step = 80: Average Return = -3.9603965282440186\n",
      "step = 90: loss = -4.2075114250183105\n",
      "step = 100: loss = -6.927062511444092\n",
      "step = 100: Average Return = -4.7990803718566895\n"
     ]
    }
   ],
   "source": [
    "for _ in range(number_iterations):\n",
    "  # Collect a few episodes using collect_policy and save to the replay buffer.\n",
    "  for _ in range(10):\n",
    "        collect_step(train_env_tf, tf_agent.collect_policy, replay_buffer)\n",
    "\n",
    "  experience, unused_info = next(iterator)\n",
    "\n",
    "  train_loss = tf_agent.train(experience).loss\n",
    "\n",
    "  step = tf_agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env_tf, tf_agent.policy, number_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "    returns.append(avg_return)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-5.0, 5.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "steps = range(0, number_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=5)\n",
    "plt.ylim(bottom=-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "def compute_animation_frames(environment, policy, size_episodes=1):\n",
    "    frame = []\n",
    "    for _ in range(size_episodes):\n",
    "        time_step = environment.reset()\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            img = environment.render()\n",
    "            frame.append(img)\n",
    "    return frame\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilts import plot_animation\n",
    "frames = compute_animation_frames(eval_env_py,tf_agent.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.FuncAnimation at 0x20c086ee860>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_animation(frames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
