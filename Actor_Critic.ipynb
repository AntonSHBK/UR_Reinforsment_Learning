{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=18)\n",
    "mpl.rc('xtick', labelsize=18)\n",
    "mpl.rc('ytick', labelsize=18)\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import collections\n",
    "import statistics\n",
    "import tensorflow as tf\n",
    "from typing import Any, List, Sequence, Tuple\n",
    "\n",
    "# from tf_env.UR_ENV import UR_env\n",
    "from tf_env.UR_ENV import UR_env_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_iterations = 100 # @param {type:\"integer\"}\n",
    "max_anglular_velocity = 4\n",
    "max_steps_env=40\n",
    "discount = 0.99 # @param {type:\"number\"}\n",
    "\n",
    "\n",
    "fc_layer_params=(300,300)\n",
    "learning_rate = 1e-5 # @param {type:\"number\"}\n",
    "number_eval_episodes = 3 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 5 # @param {type:\"integer\"}\n",
    "# collect_max_steps = 20 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = max_steps_env*collect_episodes_per_iteration # @param {type:\"integer\"}\n",
    "log_interval = 1 # @param {type:\"integer\"}\n",
    "eval_interval = 5 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "eps = np.finfo(np.float32).eps.item()# заменяет числа 0< минимальным неотрицательным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\work\\ENV\\tf_env\\UR_ENV.py:133: UserWarning: Gimbal lock detected. Setting third angle to zero since it is not possible to uniquely determine all angles.\n",
      "  j_orient =orientation.as_euler('ZXZ',degrees=True)\n"
     ]
    }
   ],
   "source": [
    "env= UR_env_simple(\n",
    "    max_steps=max_steps_env,\n",
    "    max_anglular_velocity=max_anglular_velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_actions: int, \n",
    "        num_hidden_units: int):\n",
    "        super().__init__()\n",
    "        self.common = tf.keras.layers.Dense(num_hidden_units, activation = \"relu\")\n",
    "        self.actor = tf.keras.layers.Dense(num_actions)\n",
    "        self.critic = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        x = self.common(inputs)\n",
    "        return self.actor(x), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 6\n",
    "num_hidden_units = 128\n",
    "\n",
    "model = ActorCritic(num_actions, num_hidden_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_step(action, env):\n",
    "    state, reward, done, discount = env.step(action)\n",
    "    state = np.array(state,dtype=np.float32)\n",
    "    reward = np.array(reward,dtype=np.float32)\n",
    "    done = np.array(done,dtype=np.int32)# 0 first; 1 midle; 2 last\n",
    "    discount = np.array(reward,dtype=np.float32)\n",
    "    return (state,reward,done, discount)\n",
    "\n",
    "def tf_env_step(action: tf.Tensor, env):\n",
    "  return tf.numpy_function(env_step, [action, env], \n",
    "                           [tf.float32, tf.float32, tf.int32, tf.float32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "# initial_state = env.reset()\n",
    "# initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# state = initial_state\n",
    "# print(state)\n",
    "# state=tf.expand_dims(state,0)\n",
    "# # state =tf.constant([[10.,20.,30.,10.,20.,30.]])\n",
    "\n",
    "# for i in [1,2,3,4]:\n",
    "#     action_logits_t, value = model(state)\n",
    "#     print(action_logits_t)\n",
    "#     print(value)\n",
    "# action_logits_t, value = model(state)\n",
    "# print(action_logits_t)\n",
    "# print(value)\n",
    "\n",
    "# action = np.clip(action_logits_t,-1,1)\n",
    "# print(action)\n",
    "# action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "# print(action_probs_t)\n",
    "\n",
    "# action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "# action_probs = action_probs.write(0, action_probs_t[0])\n",
    "# print(action_probs.read(0))\n",
    "\n",
    "# values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "# values = values.write(0, tf.squeeze(value[0]))\n",
    "# print(values.read(0))\n",
    "\n",
    "# state, reward, done, discount = tf_env_step(action, env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(\n",
    "    initial_state: tf.Tensor,  \n",
    "    model: tf.keras.Model, \n",
    "    max_steps: int,\n",
    "    env) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "\n",
    "    action_probs = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    values = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "    rewards = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "    # state = tf.expand_dims(initial_state, 0)\n",
    "    state = initial_state\n",
    "\n",
    "    for t in tf.range(max_steps):\n",
    "        # Convert state into a batched tensor (batch size = 1)        \n",
    "        state = tf.expand_dims(state, 0)\n",
    "        # Run the model and to get action probabilities and critic value\n",
    "        action_logits_t, value = model(state)\n",
    "\n",
    "        # Sample next action from the action probability distribution\n",
    "        \n",
    "        action = np.clip(action_logits_t,-1,1)      \n",
    "        # action = tf.random.categorical(action_logits_t, 1)[0]\n",
    "        action_probs_t = tf.nn.softmax(action_logits_t)\n",
    "\n",
    "        # Store critic values\n",
    "        values = values.write(t, tf.squeeze(value[0]))\n",
    "\n",
    "        # Store log probability of the action chosen\n",
    "        action_probs = action_probs.write(t, action_probs_t[0])\n",
    "\n",
    "        # Apply action to the environment to get next state and reward\n",
    "        state, reward, done, discount =env.step(action[0])\n",
    "        # state.set_shape(initial_state_shape)\n",
    "\n",
    "        # Store reward\n",
    "        rewards = rewards.write(t, reward)\n",
    "\n",
    "        if done==2:\n",
    "            break\n",
    "\n",
    "    action_probs = action_probs.stack()\n",
    "    values = values.stack()\n",
    "    rewards = rewards.stack()\n",
    "\n",
    "    return action_probs, values, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expected_return(\n",
    "    rewards: tf.Tensor, \n",
    "    gamma: float, \n",
    "    standardize: bool = True) -> tf.Tensor:\n",
    "  \"\"\"Compute expected returns per timestep.\"\"\"\n",
    "\n",
    "  n = tf.shape(rewards)[0]\n",
    "  returns = tf.TensorArray(dtype=tf.float32, size=n)\n",
    "\n",
    "  # Start from the end of `rewards` and accumulate reward sums\n",
    "  # into the `returns` array\n",
    "  rewards = tf.cast(rewards[::-1], dtype=tf.float32)\n",
    "  discounted_sum = tf.constant(0.0)\n",
    "  discounted_sum_shape = discounted_sum.shape\n",
    "  for i in tf.range(n):\n",
    "    reward = rewards[i]\n",
    "    discounted_sum = reward + gamma * discounted_sum\n",
    "    discounted_sum.set_shape(discounted_sum_shape)\n",
    "    returns = returns.write(i, discounted_sum)\n",
    "  returns = returns.stack()[::-1]\n",
    "\n",
    "  if standardize:\n",
    "    returns = ((returns - tf.math.reduce_mean(returns)) / \n",
    "               (tf.math.reduce_std(returns) + eps))\n",
    "\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
    "\n",
    "def compute_loss(\n",
    "    action_probs: tf.Tensor,  \n",
    "    values: tf.Tensor,  \n",
    "    returns: tf.Tensor) -> tf.Tensor:\n",
    "  \"\"\"Computes the combined Actor-Critic loss.\"\"\"\n",
    "\n",
    "  advantage = returns - values\n",
    "\n",
    "  action_log_probs = tf.math.log(action_probs)\n",
    "\n",
    "  actor_loss = -tf.math.reduce_sum(action_log_probs)\n",
    "\n",
    "\n",
    "  critic_loss = huber_loss(values, returns)\n",
    "\n",
    "  return actor_loss + critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def train_step(\n",
    "    initial_state: tf.Tensor, \n",
    "    model: tf.keras.Model, \n",
    "    optimizer: tf.keras.optimizers.Optimizer, \n",
    "    gamma: float, \n",
    "    max_steps_per_episode: int,\n",
    "    env) -> tf.Tensor:\n",
    "  \"\"\"Runs a model training step.\"\"\"\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "\n",
    "    # Run the model for one episode to collect training data\n",
    "    action_probs, values, rewards = run_episode(\n",
    "        initial_state, model, max_steps_per_episode, env) \n",
    "\n",
    "    # Calculate the expected returns\n",
    "    # returns = get_expected_return(rewards, gamma)\n",
    "    returns = rewards\n",
    "\n",
    "    # Convert training data to appropriate TF tensor shapes\n",
    "    # action_probs, values, returns = [\n",
    "    #     tf.expand_dims(x, 1) for x in [action_probs, values, returns]] \n",
    "\n",
    "    # Calculate the loss values to update our network\n",
    "    loss = compute_loss(action_probs, values, returns)\n",
    "\n",
    "  # Compute the gradients from the loss\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "  # Apply the gradients to the model's parameters\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  last_reward = rewards[-1]\n",
    "\n",
    "  return last_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_episodes = 2\n",
    "# max_steps_per_episode = 2\n",
    "\n",
    "\n",
    "\n",
    "# # The discount factor for future rewards\n",
    "# gamma = 0.99\n",
    "# initial_state = env.reset()\n",
    "# initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "# episode_reward = train_step(\n",
    "#     initial_state, \n",
    "#     model, \n",
    "#     optimizer, \n",
    "#     gamma, \n",
    "#     max_steps_per_episode,\n",
    "#     env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.00914364, shape=(), dtype=float32) dfgdfdf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [77], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m initial_state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     16\u001b[0m initial_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(initial_state, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 17\u001b[0m episode_reward \u001b[39m=\u001b[39m train_step(\n\u001b[0;32m     18\u001b[0m     initial_state, \n\u001b[0;32m     19\u001b[0m     model, \n\u001b[0;32m     20\u001b[0m     optimizer, \n\u001b[0;32m     21\u001b[0m     gamma, \n\u001b[0;32m     22\u001b[0m     max_steps_per_episode,\n\u001b[0;32m     23\u001b[0m     env)\n\u001b[0;32m     25\u001b[0m episodes_reward\u001b[39m.\u001b[39mappend(episode_reward)\n\u001b[0;32m     27\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn [75], line 17\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(initial_state, model, optimizer, gamma, max_steps_per_episode, env)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m\"\"\"Runs a model training step.\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m   \u001b[39m# Run the model for one episode to collect training data\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m   action_probs, values, rewards \u001b[39m=\u001b[39m run_episode(\n\u001b[0;32m     18\u001b[0m       initial_state, model, max_steps_per_episode, env) \n\u001b[0;32m     20\u001b[0m   \u001b[39m# Calculate the expected returns\u001b[39;00m\n\u001b[0;32m     21\u001b[0m   \u001b[39m# returns = get_expected_return(rewards, gamma)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m   returns \u001b[39m=\u001b[39m rewards\n",
      "Cell \u001b[1;32mIn [72], line 33\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(initial_state, model, max_steps, env)\u001b[0m\n\u001b[0;32m     30\u001b[0m action_probs \u001b[39m=\u001b[39m action_probs\u001b[39m.\u001b[39mwrite(t, action_probs_t[\u001b[39m0\u001b[39m])\n\u001b[0;32m     32\u001b[0m \u001b[39m# Apply action to the environment to get next state and reward\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m state, reward, done, discount \u001b[39m=\u001b[39menv\u001b[39m.\u001b[39;49mstep(action[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m     34\u001b[0m \u001b[39m# state.set_shape(initial_state_shape)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[39m# Store reward\u001b[39;00m\n\u001b[0;32m     37\u001b[0m rewards \u001b[39m=\u001b[39m rewards\u001b[39m.\u001b[39mwrite(t, reward)\n",
      "File \u001b[1;32me:\\work\\ENV\\tf_env\\UR_ENV.py:200\u001b[0m, in \u001b[0;36mUR_env_simple.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[39melif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joints_angles[index]\u001b[39m<\u001b[39m\u001b[39m-\u001b[39m\u001b[39m180\u001b[39m):\n\u001b[0;32m    198\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joints_angles[index]\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joints_angles[index]\u001b[39m+\u001b[39m\u001b[39m360\u001b[39m\n\u001b[1;32m--> 200\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__forward_kinematic_ur3(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_joints_angles)\n\u001b[0;32m    201\u001b[0m reward\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__find_reward()\n\u001b[0;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mreshape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state,\u001b[39m6\u001b[39m), reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_episode_ended, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount\n",
      "File \u001b[1;32me:\\work\\ENV\\tf_env\\UR_ENV.py:127\u001b[0m, in \u001b[0;36mUR_env_simple.__forward_kinematic_ur3\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m    125\u001b[0m base_angle\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_joint_rotation)\n\u001b[0;32m    126\u001b[0m \u001b[39mfor\u001b[39;00m index, param \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(parameters):\n\u001b[1;32m--> 127\u001b[0m     base_angle[index][\u001b[39m2\u001b[39;49m]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39mparam\n\u001b[0;32m    128\u001b[0m     new_rotation\u001b[39m=\u001b[39mrotate\u001b[39m.\u001b[39mfrom_euler(\u001b[39m'\u001b[39m\u001b[39mZXZ\u001b[39m\u001b[39m'\u001b[39m,base_angle[index],degrees\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    129\u001b[0m     orientation:rotate\u001b[39m=\u001b[39morientation\u001b[39m*\u001b[39mnew_rotation\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "model = ActorCritic(num_actions, num_hidden_units)\n",
    "\n",
    "max_episodes = 2\n",
    "max_steps_per_episode = 2\n",
    "\n",
    "\n",
    "# The discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Keep the last episodes reward\n",
    "episodes_reward = []\n",
    "\n",
    "for i in tf.range(max_episodes):\n",
    "    initial_state = env.reset()\n",
    "    initial_state = tf.constant(initial_state, dtype=tf.float32)\n",
    "    episode_reward = train_step(\n",
    "        initial_state, \n",
    "        model, \n",
    "        optimizer, \n",
    "        gamma, \n",
    "        max_steps_per_episode,\n",
    "        env)\n",
    "\n",
    "    episodes_reward.append(episode_reward)\n",
    "\n",
    "    if i % 1 == 0:\n",
    "      print(episode_reward, 'dfgdfdf')\n",
    "      pass # print(f'Episode {i}: average reward: {avg_reward}')\n",
    "\n",
    "\n",
    "# print(f'\\nSolved at episode {i}: average reward: {running_reward:.2f}!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
