{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_agents\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tf_agents.environments import tf_py_environment\n",
    "\n",
    "from tf_env.UR_ENV import UR_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env_py= UR_env()\n",
    "eval_env_py=UR_env()\n",
    "\n",
    "train_env_tf=tf_py_environment.TFPyEnvironment(train_env_py)\n",
    "eval_env_tf=tf_py_environment.TFPyEnvironment(eval_env_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "from tf_agents.train.utils import strategy_utils\n",
    "use_gpu = True\n",
    "strategy = strategy_utils.get_strategy(tpu=False, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.agents.ddpg import critic_network\n",
    "\n",
    "critic_joint_fc_layer_params = (256, 256)\n",
    "\n",
    "observation_spec, action_spec, time_step_spec = (\n",
    "      spec_utils.get_tensor_specs(train_env_tf))\n",
    "\n",
    "with strategy.scope():\n",
    "  critic_net = critic_network.CriticNetwork(\n",
    "        (observation_spec, action_spec),\n",
    "        observation_fc_layer_params=None,\n",
    "        action_fc_layer_params=None,\n",
    "        joint_fc_layer_params=critic_joint_fc_layer_params,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        last_kernel_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.agents.sac import tanh_normal_projection_network\n",
    "\n",
    "actor_fc_layer_params = (256, 256)\n",
    "\n",
    "with strategy.scope():\n",
    "  actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "      observation_spec,\n",
    "      action_spec,\n",
    "      fc_layer_params=actor_fc_layer_params,\n",
    "      continuous_projection_net=(\n",
    "          tanh_normal_projection_network.TanhNormalProjectionNetwork))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.agents.sac import sac_agent\n",
    "\n",
    "critic_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "actor_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "alpha_learning_rate = 3e-4 # @param {type:\"number\"}\n",
    "target_update_tau = 0.005 # @param {type:\"number\"}\n",
    "target_update_period = 1 # @param {type:\"number\"}\n",
    "gamma = 0.99 # @param {type:\"number\"}\n",
    "reward_scale_factor = 1.0 # @param {type:\"number\"}\n",
    "\n",
    "with strategy.scope():\n",
    "  train_step = train_utils.create_train_step()\n",
    "\n",
    "  tf_agent = sac_agent.SacAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network=actor_net,\n",
    "        critic_network=critic_net,\n",
    "        actor_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=actor_learning_rate),\n",
    "        critic_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=critic_learning_rate),\n",
    "        alpha_optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=alpha_learning_rate),\n",
    "        target_update_tau=target_update_tau,\n",
    "        target_update_period=target_update_period,\n",
    "        td_errors_loss_fn=tf.math.squared_difference,\n",
    "        gamma=gamma,\n",
    "        reward_scale_factor=reward_scale_factor,\n",
    "        train_step_counter=train_step)\n",
    "\n",
    "  tf_agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "\n",
    "replay_buffer_capacity = 100 # @param {type:\"integer\"}\n",
    "batch_size = 50 # @param {type:\"integer\"}\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    tf_agent.collect_data_spec,\n",
    "    batch_size=batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "replay_observer = [replay_buffer.add_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import py_tf_eager_policy\n",
    "\n",
    "tf_eval_policy = tf_agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_collect_policy = tf_agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(\n",
    "  tf_collect_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_py_policy\n",
    "\n",
    "random_policy = random_py_policy.RandomPyPolicy(\n",
    "  train_env_py.time_step_spec(), train_env_py.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__ResourceScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [2,3], indices.shape [50], params.shape [5000,2,3] [Op:ResourceScatterUpdate]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [60], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m initial_collect_steps \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m \u001b[39m# @param {type:\"integer\"}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m initial_collect_actor \u001b[39m=\u001b[39m actor\u001b[39m.\u001b[39mActor(\n\u001b[0;32m      6\u001b[0m   train_env_py,\n\u001b[0;32m      7\u001b[0m   random_policy,\n\u001b[0;32m      8\u001b[0m   train_step,\n\u001b[0;32m      9\u001b[0m   steps_per_run\u001b[39m=\u001b[39minitial_collect_steps,\n\u001b[0;32m     10\u001b[0m   observers\u001b[39m=\u001b[39mreplay_observer)\n\u001b[1;32m---> 11\u001b[0m initial_collect_actor\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\train\\actor.py:152\u001b[0m, in \u001b[0;36mActor.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 152\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time_step, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_policy_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_driver\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m    153\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_time_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_policy_state)\n\u001b[0;32m    155\u001b[0m   \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_write_summaries \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_interval \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    156\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_summary \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_summary_interval):\n\u001b[0;32m    157\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_metric_summaries()\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\drivers\\py_driver.py:126\u001b[0m, in \u001b[0;36mPyDriver.run\u001b[1;34m(self, time_step, policy_state)\u001b[0m\n\u001b[0;32m    124\u001b[0m   observer((time_step, action_step_with_previous_state, next_time_step))\n\u001b[0;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m observer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservers:\n\u001b[1;32m--> 126\u001b[0m   observer(traj)\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_end_episode_on_boundary:\n\u001b[0;32m    129\u001b[0m   num_episodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(traj\u001b[39m.\u001b[39mis_boundary())\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\replay_buffer.py:83\u001b[0m, in \u001b[0;36mReplayBuffer.add_batch\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd_batch\u001b[39m(\u001b[39mself\u001b[39m, items):\n\u001b[0;32m     73\u001b[0m   \u001b[39m\"\"\"Adds a batch of items to the replay buffer.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39m    Adds `items` to the replay buffer.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_add_batch(items)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\tf_uniform_replay_buffer.py:205\u001b[0m, in \u001b[0;36mTFUniformReplayBuffer._add_batch\u001b[1;34m(self, items)\u001b[0m\n\u001b[0;32m    203\u001b[0m write_rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_rows_for_id(id_)\n\u001b[0;32m    204\u001b[0m write_id_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id_table\u001b[39m.\u001b[39mwrite(write_rows, id_)\n\u001b[1;32m--> 205\u001b[0m write_data_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_table\u001b[39m.\u001b[39;49mwrite(write_rows, items)\n\u001b[0;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mgroup(write_id_op, write_data_op)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\table.py:128\u001b[0m, in \u001b[0;36mTable.write\u001b[1;34m(self, rows, values, slots)\u001b[0m\n\u001b[0;32m    126\u001b[0m flattened_slots \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(slots)\n\u001b[0;32m    127\u001b[0m flattened_values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(values)\n\u001b[1;32m--> 128\u001b[0m write_ops \u001b[39m=\u001b[39m [\n\u001b[0;32m    129\u001b[0m     tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mscatter_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slot2storage_map[slot], rows,\n\u001b[0;32m    130\u001b[0m                                 value)\u001b[39m.\u001b[39mop\n\u001b[0;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m (slot, value) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flattened_slots, flattened_values)\n\u001b[0;32m    132\u001b[0m ]\n\u001b[0;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mgroup(\u001b[39m*\u001b[39mwrite_ops)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tf_agents\\replay_buffers\\table.py:129\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    126\u001b[0m flattened_slots \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(slots)\n\u001b[0;32m    127\u001b[0m flattened_values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(values)\n\u001b[0;32m    128\u001b[0m write_ops \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 129\u001b[0m     tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mscatter_update(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot2storage_map[slot], rows,\n\u001b[0;32m    130\u001b[0m                                 value)\u001b[39m.\u001b[39mop\n\u001b[0;32m    131\u001b[0m     \u001b[39mfor\u001b[39;00m (slot, value) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(flattened_slots, flattened_values)\n\u001b[0;32m    132\u001b[0m ]\n\u001b[0;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mgroup(\u001b[39m*\u001b[39mwrite_ops)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py:431\u001b[0m, in \u001b[0;36mscatter_update\u001b[1;34m(ref, indices, updates, use_locking, name)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[39mif\u001b[39;00m ref\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39m_is_ref_dtype:\n\u001b[0;32m    429\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_state_ops\u001b[39m.\u001b[39mscatter_update(ref, indices, updates,\n\u001b[0;32m    430\u001b[0m                                       use_locking\u001b[39m=\u001b[39muse_locking, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m--> 431\u001b[0m \u001b[39mreturn\u001b[39;00m ref\u001b[39m.\u001b[39m_lazy_read(gen_resource_variable_ops\u001b[39m.\u001b[39;49mresource_scatter_update(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    432\u001b[0m     ref\u001b[39m.\u001b[39;49mhandle, indices, ops\u001b[39m.\u001b[39;49mconvert_to_tensor(updates, ref\u001b[39m.\u001b[39;49mdtype),\n\u001b[0;32m    433\u001b[0m     name\u001b[39m=\u001b[39;49mname))\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:1174\u001b[0m, in \u001b[0;36mresource_scatter_update\u001b[1;34m(resource, indices, updates, name)\u001b[0m\n\u001b[0;32m   1172\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1173\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1174\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[0;32m   1175\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[0;32m   1176\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__ResourceScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} Must have updates.shape = indices.shape + params.shape[1:] or updates.shape = [], got updates.shape [2,3], indices.shape [50], params.shape [5000,2,3] [Op:ResourceScatterUpdate]"
     ]
    }
   ],
   "source": [
    "from tf_agents.train import actor\n",
    "\n",
    "initial_collect_steps = 100 # @param {type:\"integer\"}\n",
    "\n",
    "bf = []\n",
    "\n",
    "br=[bf.append]\n",
    "\n",
    "initial_collect_actor = actor.Actor(\n",
    "  train_env_py,\n",
    "  random_policy,\n",
    "  train_step,\n",
    "  steps_per_run=initial_collect_steps,\n",
    "  observers=br)\n",
    "initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
