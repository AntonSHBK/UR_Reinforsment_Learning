{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_env():\n",
    "    def __init__(self, step_size=13,targets=[166,-155],max_steps=100):\n",
    "        self.pos=np.random.uniform(-10,10,len(targets))\n",
    "        self.max_steps=max_steps\n",
    "        self.step_size=step_size\n",
    "        self.targets=targets\n",
    "        self.max_num=1000\n",
    "        self.min_num=-1000\n",
    "        self.done=False\n",
    "        self.def_size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.counter_stop=0\n",
    "        self.obsevation_space=len(targets)\n",
    "        self.action_space=len(targets)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos=np.random.uniform(-10,10,len(self.targets))\n",
    "        self.size_pos=[abs(self.targets[i]-self.pos[i]) for i in range(len(self.targets))]\n",
    "        self.counter_stop=0\n",
    "        return self.pos\n",
    "\n",
    "    def step(self,action):\n",
    "        self.counter_stop+=1\n",
    "        if self.counter_stop>=self.max_steps:\n",
    "            self.done=True\n",
    "        self.last_pos=np.copy(self.pos) \n",
    "        self.last_size_pos=np.copy(self.size_pos)\n",
    "        self.reward=np.zeros(self.pos.shape)\n",
    "        for i in range(len(action)):\n",
    "            self.pos[i]=self.pos[i]+(action[i]*self.step_size)\n",
    "            self.size_pos[i]=abs(self.targets[i]-self.pos[i])\n",
    "            difference= self.last_size_pos[i]-self.size_pos[i]\n",
    "            self.reward[i]=difference/self.def_size_pos[i]\n",
    "        return self.pos, self.reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(obs_space, action_space):\n",
    "    # last_init=kernel_initializer= tf.keras.initializers.glorot_uniform()\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "    inputs = tf.keras.layers.Input(shape=(obs_space,))\n",
    "    out = tf.keras.layers.Dense(4, activation=\"relu\")(inputs)\n",
    "    out = tf.keras.layers.Dense(4, activation=\"relu\")(inputs)\n",
    "    outputs = tf.keras.layers.Dense(action_space, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(obs, model, lower_bound=-1, upper_bound=1):\n",
    "    logits=model(obs)\n",
    "    sampled_actions = logits.numpy()\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    # legal_action = sampled_actions\n",
    "    return logits, legal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "    with tf.GradientTape() as tape:        \n",
    "        logits, action = policy(obs, model)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "        loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return next_obs, reward, done, grads, action, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(env, num_steps, model, loss_fn):\n",
    "    all_rewarsd=[]\n",
    "    all_grads=[]\n",
    "    all_obs=[]\n",
    "    all_action=[]\n",
    "    all_obs_prev=[]\n",
    "    all_losses=[]\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        all_obs_prev.append(np.copy(obs))\n",
    "        obs, reward, dode, grads, action, loss = play_one_step(env,obs,model,loss_fn)\n",
    "        all_rewarsd.append(np.copy(reward))\n",
    "        all_grads.append(np.copy(grads))\n",
    "        all_obs.append(np.copy(obs))\n",
    "        all_action.append(np.copy(action))\n",
    "        all_losses.append(np.copy(loss))\n",
    "    return all_rewarsd, all_grads, all_obs, all_action,all_obs_prev, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount):\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i]=np.mean(rewards[i])*pow(discount,i)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iteration_learn = 10\n",
    "n_max_steps = 10\n",
    "discount = 0.95\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=simple_env()\n",
    "first_pos=env.reset()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 4)                 12        \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_25/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.5131767 ,  0.06237054, -0.69414616, -0.8847337 ],\n",
      "       [-0.5437341 ,  0.7508378 , -0.7935047 ,  0.24884486]],\n",
      "      dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_25/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_26/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[ 0.09577108,  0.06470997],\n",
      "       [-0.05173025,  0.06546802],\n",
      "       [ 0.04180687, -0.04479575],\n",
      "       [ 0.03161781, -0.05337641]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_26/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for var in model.trainable_variables:\n",
    "  print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[<tf.Variable 'dense_25/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.5131767 ,  0.06237054, -0.69414616, -0.8847337 ],\n",
      "       [-0.5437341 ,  0.7508378 , -0.7935047 ,  0.24884486]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_25/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "[<tf.Variable 'dense_26/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[ 0.09577108,  0.06470997],\n",
      "       [-0.05173025,  0.06546802],\n",
      "       [ 0.04180687, -0.04479575],\n",
      "       [ 0.03161781, -0.05337641]], dtype=float32)>, <tf.Variable 'dense_26/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weights)\n",
    "print(model.layers[1].weights)\n",
    "print(model.layers[2].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env=simple_env()\n",
    "# model = get_actor(env.obsevation_space,env.action_space)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# obs = env.reset()\n",
    "# # obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "# obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "# with tf.GradientTape() as tape:        \n",
    "#     action = policy(obs,model)\n",
    "#     next_obs, reward, done = env.step(action)\n",
    "#     # print(obs)\n",
    "#     # print(next_obs)   \n",
    "#     # print(action)\n",
    "#     print(reward)\n",
    "#     logits = model(obs)\n",
    "#     y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "#     loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "# grads = tape.gradient(loss, model.trainable_variables)\n",
    "# # print(grads)\n",
    "# # for i in range(len(model.trainable_variables)):\n",
    "# #     rd=np.mean(reward)\n",
    "# #     grads[i]=tf.math.multiply(grads[i],rd)\n",
    "\n",
    "# optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "# print(\"*********************************************\")\n",
    "# print(grads)\n",
    "# print(\"*********************************************\")\n",
    "# for var in model.trainable_variables:\n",
    "#   print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Reward is ==> 0.035648167430368735\n",
      "Episode * 1 * Reward is ==> 0.022746634507700182\n",
      "Episode * 2 * Reward is ==> -0.06857185578493476\n",
      "Episode * 3 * Reward is ==> 0.03306106318444632\n",
      "Episode * 4 * Reward is ==> 0.014610207784938775\n",
      "Episode * 5 * Reward is ==> 0.013220083396232684\n",
      "Episode * 6 * Reward is ==> -0.5001934146315604\n",
      "Episode * 7 * Reward is ==> 0.008983750291591848\n",
      "Episode * 8 * Reward is ==> -0.0036250848426381804\n",
      "Episode * 9 * Reward is ==> 0.014631918246855589\n",
      "Episode * 10 * Reward is ==> -0.3825555201129033\n",
      "Episode * 11 * Reward is ==> -0.5185346505351036\n",
      "Episode * 12 * Reward is ==> 0.006447602954804326\n",
      "Episode * 13 * Reward is ==> -0.6050961371171932\n",
      "Episode * 14 * Reward is ==> -0.43616125914093695\n",
      "Episode * 15 * Reward is ==> -0.5620204793749071\n",
      "Episode * 16 * Reward is ==> -0.5973259100582606\n",
      "Episode * 17 * Reward is ==> 0.03874488623312579\n",
      "Episode * 18 * Reward is ==> -0.00931912990303825\n",
      "Episode * 19 * Reward is ==> -0.6211298314039662\n",
      "Episode * 20 * Reward is ==> -0.6142733443192793\n",
      "Episode * 21 * Reward is ==> -0.578990274451694\n",
      "Episode * 22 * Reward is ==> -0.5585404740053032\n",
      "Episode * 23 * Reward is ==> -0.01957019664704936\n",
      "Episode * 24 * Reward is ==> -0.5641947992186274\n",
      "Episode * 25 * Reward is ==> 0.06554315615383213\n",
      "Episode * 26 * Reward is ==> 0.08574862850511847\n",
      "Episode * 27 * Reward is ==> 0.0894779079220587\n",
      "Episode * 28 * Reward is ==> 0.052724442151600825\n",
      "Episode * 29 * Reward is ==> -0.6237989044134733\n",
      "Episode * 30 * Reward is ==> -0.6285209104025773\n",
      "Episode * 31 * Reward is ==> -0.6325028843251701\n",
      "Episode * 32 * Reward is ==> -0.6365071160604543\n",
      "Episode * 33 * Reward is ==> 0.11338910172555981\n",
      "Episode * 34 * Reward is ==> 0.11659903589325529\n",
      "Episode * 35 * Reward is ==> -0.6171725030427448\n",
      "Episode * 36 * Reward is ==> -0.6260214013287432\n",
      "Episode * 37 * Reward is ==> -0.5971374417673867\n",
      "Episode * 38 * Reward is ==> 0.13355818404086042\n",
      "Episode * 39 * Reward is ==> -0.6163559831686277\n",
      "Episode * 40 * Reward is ==> -0.6363046155171699\n",
      "Episode * 41 * Reward is ==> -0.6269427412814169\n",
      "Episode * 42 * Reward is ==> -0.6383913328024315\n",
      "Episode * 43 * Reward is ==> -0.6284933803466419\n",
      "Episode * 44 * Reward is ==> -0.6310260258321905\n",
      "Episode * 45 * Reward is ==> -0.5668765042129673\n",
      "Episode * 46 * Reward is ==> -0.6383636314169127\n",
      "Episode * 47 * Reward is ==> -0.6370824663956961\n",
      "Episode * 48 * Reward is ==> -0.04761387990498313\n",
      "Episode * 49 * Reward is ==> -0.533864028755797\n",
      "Episode * 50 * Reward is ==> -0.6385010931134287\n",
      "Episode * 51 * Reward is ==> 0.18798970999133535\n",
      "Episode * 52 * Reward is ==> -0.6381856160131424\n",
      "Episode * 53 * Reward is ==> -0.6023376293477529\n",
      "Episode * 54 * Reward is ==> -0.23249708710288544\n",
      "Episode * 55 * Reward is ==> -0.6112786289778194\n",
      "Episode * 56 * Reward is ==> -0.638330138880554\n",
      "Episode * 57 * Reward is ==> -0.5794599558321951\n",
      "Episode * 58 * Reward is ==> 0.014795920796065983\n",
      "Episode * 59 * Reward is ==> -0.23794384826188772\n",
      "Episode * 60 * Reward is ==> -0.6337520535920103\n",
      "Episode * 61 * Reward is ==> 0.18184990034420723\n",
      "Episode * 62 * Reward is ==> -0.6335470314406509\n",
      "Episode * 63 * Reward is ==> -0.6382320233899104\n",
      "Episode * 64 * Reward is ==> -0.631753295741698\n",
      "Episode * 65 * Reward is ==> -0.6346639428767515\n",
      "Episode * 66 * Reward is ==> -0.6324393946184259\n",
      "Episode * 67 * Reward is ==> -0.5980922599766388\n",
      "Episode * 68 * Reward is ==> -0.6320427021660477\n",
      "Episode * 69 * Reward is ==> -0.5896682347152022\n",
      "Episode * 70 * Reward is ==> 0.22477409527894857\n",
      "Episode * 71 * Reward is ==> -0.6342358537369213\n",
      "Episode * 72 * Reward is ==> -0.6328224332189603\n",
      "Episode * 73 * Reward is ==> -0.638482162149082\n",
      "Episode * 74 * Reward is ==> -0.6044724184512111\n",
      "Episode * 75 * Reward is ==> -0.04098212468534515\n",
      "Episode * 76 * Reward is ==> -0.6345624042214577\n",
      "Episode * 77 * Reward is ==> -0.6348409671702415\n",
      "Episode * 78 * Reward is ==> -0.6319337515187433\n",
      "Episode * 79 * Reward is ==> -0.6361371249221792\n",
      "Episode * 80 * Reward is ==> -0.6371559252671284\n",
      "Episode * 81 * Reward is ==> -0.6384992929542179\n",
      "Episode * 82 * Reward is ==> -0.6370236115904466\n",
      "Episode * 83 * Reward is ==> -0.5154260839591865\n",
      "Episode * 84 * Reward is ==> -0.2653485286773673\n",
      "Episode * 85 * Reward is ==> -0.6347725164956683\n",
      "Episode * 86 * Reward is ==> -0.6001011091400049\n",
      "Episode * 87 * Reward is ==> -0.1035834519120623\n",
      "Episode * 88 * Reward is ==> -0.6290200993415827\n",
      "Episode * 89 * Reward is ==> -0.5960660523538273\n",
      "Episode * 90 * Reward is ==> -0.6295825506365935\n",
      "Episode * 91 * Reward is ==> -0.6379901513183632\n",
      "Episode * 92 * Reward is ==> -0.6378184674405949\n",
      "Episode * 93 * Reward is ==> -0.6386207888075204\n",
      "Episode * 94 * Reward is ==> -0.6212399543093623\n",
      "Episode * 95 * Reward is ==> -0.6385567235359103\n",
      "Episode * 96 * Reward is ==> -0.6294088466275779\n",
      "Episode * 97 * Reward is ==> -0.6381096504543846\n",
      "Episode * 98 * Reward is ==> 0.035471428082712184\n",
      "Episode * 99 * Reward is ==> -0.6158430912294897\n",
      "Episode * 100 * Reward is ==> -0.6386188710926678\n",
      "Episode * 101 * Reward is ==> -0.6381950719671938\n",
      "Episode * 102 * Reward is ==> -0.0031764239574269965\n",
      "Episode * 103 * Reward is ==> -0.638111015207051\n",
      "Episode * 104 * Reward is ==> -0.6357999129306644\n",
      "Episode * 105 * Reward is ==> -0.6386211874596017\n",
      "Episode * 106 * Reward is ==> -0.6326916748318174\n",
      "Episode * 107 * Reward is ==> -0.6343294606861175\n",
      "Episode * 108 * Reward is ==> -0.638577670408264\n",
      "Episode * 109 * Reward is ==> -0.6287143967280636\n",
      "Episode * 110 * Reward is ==> -0.6359062684870779\n",
      "Episode * 111 * Reward is ==> -0.6142572305997116\n",
      "Episode * 112 * Reward is ==> -0.2593733951830631\n",
      "Episode * 113 * Reward is ==> -0.6385801600434762\n",
      "Episode * 114 * Reward is ==> -0.6386075100715779\n",
      "Episode * 115 * Reward is ==> -0.6369455720070033\n",
      "Episode * 116 * Reward is ==> -0.6383479990984018\n",
      "Episode * 117 * Reward is ==> 0.06823275087596457\n",
      "Episode * 118 * Reward is ==> 0.03320755125066727\n",
      "Episode * 119 * Reward is ==> -0.6386214925387648\n",
      "Episode * 120 * Reward is ==> -0.6385517992772323\n",
      "Episode * 121 * Reward is ==> -0.5146025608348722\n",
      "Episode * 122 * Reward is ==> -0.6038929613023467\n",
      "Episode * 123 * Reward is ==> -0.6384629344391081\n",
      "Episode * 124 * Reward is ==> -0.6385765872115808\n",
      "Episode * 125 * Reward is ==> -0.6375020992774897\n",
      "Episode * 126 * Reward is ==> -0.6379699015497667\n",
      "Episode * 127 * Reward is ==> -0.638620882136669\n",
      "Episode * 128 * Reward is ==> -0.6379230969496149\n",
      "Episode * 129 * Reward is ==> -0.6123487372084453\n",
      "Episode * 130 * Reward is ==> -0.4338375938541695\n",
      "Episode * 131 * Reward is ==> -0.4085777800323419\n",
      "Episode * 132 * Reward is ==> -0.6264242133074354\n",
      "Episode * 133 * Reward is ==> -0.628557119002135\n",
      "Episode * 134 * Reward is ==> -0.6385107042737281\n",
      "Episode * 135 * Reward is ==> -0.615555489301047\n",
      "Episode * 136 * Reward is ==> -0.46620130532409004\n",
      "Episode * 137 * Reward is ==> -0.6348946493364889\n",
      "Episode * 138 * Reward is ==> -0.6380526387820389\n",
      "Episode * 139 * Reward is ==> -0.6383150081547198\n",
      "Episode * 140 * Reward is ==> -0.6351954615518014\n",
      "Episode * 141 * Reward is ==> -0.6359335326832302\n",
      "Episode * 142 * Reward is ==> -0.5332950929337723\n",
      "Episode * 143 * Reward is ==> -0.6259717299147101\n",
      "Episode * 144 * Reward is ==> -0.18802545662139064\n",
      "Episode * 145 * Reward is ==> -0.6283673069679008\n",
      "Episode * 146 * Reward is ==> -0.6347639570442388\n",
      "Episode * 147 * Reward is ==> -0.5252248247756669\n",
      "Episode * 148 * Reward is ==> -0.6386200367847651\n",
      "Episode * 149 * Reward is ==> -0.6359527001495151\n",
      "Episode * 150 * Reward is ==> -0.6383178948362638\n",
      "Episode * 151 * Reward is ==> -0.6381882943196364\n",
      "Episode * 152 * Reward is ==> -0.5770808317423419\n",
      "Episode * 153 * Reward is ==> -0.5832426517538852\n",
      "Episode * 154 * Reward is ==> -0.4888428415359251\n",
      "Episode * 155 * Reward is ==> -0.63860493421807\n",
      "Episode * 156 * Reward is ==> -0.632046559249385\n",
      "Episode * 157 * Reward is ==> -0.23819659264884416\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [128], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m buffer\u001b[39m=\u001b[39m[]\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iteration_learn):\n\u001b[1;32m----> 3\u001b[0m     all_rewards, all_grads, all_obs, all_action, all_obs_prev, all_losses \u001b[39m=\u001b[39m play_episodes(env,n_max_steps,model,loss_fn)\n\u001b[0;32m      4\u001b[0m     all_final_rewards \u001b[39m=\u001b[39m discount_rewards(all_rewards,discount)\n\u001b[0;32m      5\u001b[0m     all_mean_grads \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn [120], line 11\u001b[0m, in \u001b[0;36mplay_episodes\u001b[1;34m(env, num_steps, model, loss_fn)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m     10\u001b[0m     all_obs_prev\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mcopy(obs))\n\u001b[1;32m---> 11\u001b[0m     obs, reward, dode, grads, action, loss \u001b[39m=\u001b[39m play_one_step(env,obs,model,loss_fn)\n\u001b[0;32m     12\u001b[0m     all_rewarsd\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mcopy(reward))\n\u001b[0;32m     13\u001b[0m     all_grads\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mcopy(grads))\n",
      "Cell \u001b[1;32mIn [119], line 6\u001b[0m, in \u001b[0;36mplay_one_step\u001b[1;34m(env, obs, model, loss_fn)\u001b[0m\n\u001b[0;32m      4\u001b[0m     logits, action \u001b[39m=\u001b[39m policy(obs, model)\n\u001b[0;32m      5\u001b[0m     next_obs, reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m----> 6\u001b[0m     y_target\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mreduce_mean(tf\u001b[39m.\u001b[39;49mVariable(reward))\n\u001b[0;32m      7\u001b[0m     loss\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_mean(loss_fn(y_target,logits))\n\u001b[0;32m      8\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(loss, model\u001b[39m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:266\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v1_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    265\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m Variable:\n\u001b[1;32m--> 266\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(VariableMetaclass, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:247\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v2_call\u001b[1;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m aggregation \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m   aggregation \u001b[39m=\u001b[39m VariableAggregation\u001b[39m.\u001b[39mNONE\n\u001b[1;32m--> 247\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\n\u001b[0;32m    248\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m    249\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    250\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    251\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    252\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m    254\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    255\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m    256\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    257\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    258\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m    259\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:240\u001b[0m, in \u001b[0;36mVariableMetaclass._variable_v2_call.<locals>.<lambda>\u001b[1;34m(**kws)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_variable_v2_call\u001b[39m(\u001b[39mcls\u001b[39m,\n\u001b[0;32m    227\u001b[0m                       initial_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    228\u001b[0m                       trainable\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m                       aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE,\n\u001b[0;32m    238\u001b[0m                       shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m   \u001b[39m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m   previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws: default_variable_creator_v2(\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkws)\n\u001b[0;32m    241\u001b[0m   \u001b[39mfor\u001b[39;00m _, getter \u001b[39min\u001b[39;00m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_variable_creator_stack:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:2755\u001b[0m, in \u001b[0;36mdefault_variable_creator_v2\u001b[1;34m(next_creator, **kwargs)\u001b[0m\n\u001b[0;32m   2752\u001b[0m aggregation \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39maggregation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   2753\u001b[0m shape \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 2755\u001b[0m \u001b[39mreturn\u001b[39;00m resource_variable_ops\u001b[39m.\u001b[39;49mResourceVariable(\n\u001b[0;32m   2756\u001b[0m     initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   2757\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   2758\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   2759\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   2760\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   2761\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   2762\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   2763\u001b[0m     variable_def\u001b[39m=\u001b[39;49mvariable_def,\n\u001b[0;32m   2764\u001b[0m     import_scope\u001b[39m=\u001b[39;49mimport_scope,\n\u001b[0;32m   2765\u001b[0m     distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m   2766\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   2767\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m   2768\u001b[0m     shape\u001b[39m=\u001b[39;49mshape)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py:268\u001b[0m, in \u001b[0;36mVariableMetaclass.__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_variable_v2_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(VariableMetaclass, \u001b[39mcls\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1659\u001b[0m, in \u001b[0;36mResourceVariable.__init__\u001b[1;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[0;32m   1656\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_from_proto(variable_def, import_scope\u001b[39m=\u001b[39mimport_scope,\n\u001b[0;32m   1657\u001b[0m                         validate_shape\u001b[39m=\u001b[39mvalidate_shape)\n\u001b[0;32m   1658\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1659\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_from_args(\n\u001b[0;32m   1660\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1661\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1662\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1663\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1664\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1665\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1666\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1667\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1668\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation,\n\u001b[0;32m   1669\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1670\u001b[0m       distribute_strategy\u001b[39m=\u001b[39;49mdistribute_strategy,\n\u001b[0;32m   1671\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1672\u001b[0m   )\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1827\u001b[0m, in \u001b[0;36mResourceVariable._init_from_args\u001b[1;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape, validate_shape)\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1826\u001b[0m     shape \u001b[39m=\u001b[39m initial_value\u001b[39m.\u001b[39mshape\n\u001b[1;32m-> 1827\u001b[0m   handle \u001b[39m=\u001b[39m eager_safe_variable_handle(\n\u001b[0;32m   1828\u001b[0m       initial_value\u001b[39m=\u001b[39;49minitial_value,\n\u001b[0;32m   1829\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1830\u001b[0m       shared_name\u001b[39m=\u001b[39;49mshared_name,\n\u001b[0;32m   1831\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1832\u001b[0m       graph_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_in_graph_mode)\n\u001b[0;32m   1833\u001b[0m   handle\u001b[39m.\u001b[39m_parent_trackable \u001b[39m=\u001b[39m weakref\u001b[39m.\u001b[39mref(\u001b[39mself\u001b[39m)\n\u001b[0;32m   1834\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:237\u001b[0m, in \u001b[0;36meager_safe_variable_handle\u001b[1;34m(initial_value, shape, shared_name, name, graph_mode)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39m\"\"\"Creates a variable handle with information to do shape inference.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m \u001b[39mThe dtype is read from `initial_value` and stored in the returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m  The handle, a `Tensor` of type `resource`.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m dtype \u001b[39m=\u001b[39m initial_value\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\n\u001b[1;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name,\n\u001b[0;32m    238\u001b[0m                                              graph_mode, initial_value)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:161\u001b[0m, in \u001b[0;36m_variable_handle_from_shape_and_dtype\u001b[1;34m(shape, dtype, shared_name, name, graph_mode, initial_value)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mInternalError(  \u001b[39m# pylint: disable=no-value-for-parameter\u001b[39;00m\n\u001b[0;32m    157\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing an explicit shared_name is not allowed when executing eagerly.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    159\u001b[0m   shared_name \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39manonymous_name()\n\u001b[1;32m--> 161\u001b[0m handle \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39;49mvar_handle_op(\n\u001b[0;32m    162\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    163\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    164\u001b[0m     shared_name\u001b[39m=\u001b[39;49mshared_name,\n\u001b[0;32m    165\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    166\u001b[0m     container\u001b[39m=\u001b[39;49mcontainer)\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m initial_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m   initial_value \u001b[39m=\u001b[39m handle\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:1226\u001b[0m, in \u001b[0;36mvar_handle_op\u001b[1;34m(dtype, shape, container, shared_name, allowed_devices, name)\u001b[0m\n\u001b[0;32m   1224\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   1225\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1226\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   1227\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mVarHandleOp\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39m\"\u001b[39;49m\u001b[39mcontainer\u001b[39;49m\u001b[39m\"\u001b[39;49m, container, \u001b[39m\"\u001b[39;49m\u001b[39mshared_name\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1228\u001b[0m       shared_name, \u001b[39m\"\u001b[39;49m\u001b[39mdtype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype, \u001b[39m\"\u001b[39;49m\u001b[39mshape\u001b[39;49m\u001b[39m\"\u001b[39;49m, shape, \u001b[39m\"\u001b[39;49m\u001b[39mallowed_devices\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1229\u001b[0m       allowed_devices)\n\u001b[0;32m   1230\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1231\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "buffer=[]\n",
    "for iteration in range(n_iteration_learn):\n",
    "    all_rewards, all_grads, all_obs, all_action, all_obs_prev, all_losses = play_episodes(env,n_max_steps,model,loss_fn)\n",
    "    all_final_rewards = discount_rewards(all_rewards,discount)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        var = []\n",
    "        for step, final_reward in enumerate(all_final_rewards):            \n",
    "            for i in range(len(model.trainable_variables)):\n",
    "                all_grads[step][i]=tf.math.multiply(all_grads[step][i],final_reward)\n",
    "            var.append(all_grads[step][var_index])\n",
    "        all_mean_grads.append(tf.reduce_mean(var,axis=0))\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    episode_reward=tf.reduce_sum(all_final_rewards)\n",
    "    buffer.append([all_final_rewards,all_obs,all_action])\n",
    "    print(\"Episode * {} * Reward is ==> {}\".format(iteration, episode_reward))\n",
    "\n",
    "print(\"end\")\n",
    "\n",
    "for var in model.trainable_variables:\n",
    "  print(var, \"\\n\")\n",
    "\n",
    "\n",
    "print(env.pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
