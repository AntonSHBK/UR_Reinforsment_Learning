{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_env():\n",
    "    def __init__(self, step_size=13,targets=[166,-155],max_steps=100):\n",
    "        self.pos=np.random.uniform(-10,10,len(targets))\n",
    "        self.max_steps=max_steps\n",
    "        self.step_size=step_size\n",
    "        self.targets=targets\n",
    "        self.max_num=1000\n",
    "        self.min_num=-1000\n",
    "        self.done=False\n",
    "        self.def_size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.counter_stop=0\n",
    "        self.obsevation_space=len(targets)\n",
    "        self.action_space=len(targets)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos=np.random.uniform(-10,10,len(self.targets))\n",
    "        self.size_pos=[abs(self.targets[i]-self.pos[i]) for i in range(len(self.targets))]\n",
    "        self.counter_stop=0\n",
    "        return self.pos\n",
    "\n",
    "    def step(self,action):\n",
    "        self.counter_stop+=1\n",
    "        if self.counter_stop>=self.max_steps:\n",
    "            self.done=True\n",
    "        self.last_pos=np.copy(self.pos) \n",
    "        self.last_size_pos=np.copy(self.size_pos)\n",
    "        self.reward=np.zeros(self.pos.shape)\n",
    "        for i in range(len(action)):\n",
    "            self.pos[i]=self.pos[i]+(action[i]*self.step_size)\n",
    "            self.size_pos[i]=abs(self.targets[i]-self.pos[i])\n",
    "            difference= self.last_size_pos[i]-self.size_pos[i]\n",
    "            self.reward[i]=difference/self.def_size_pos[i]\n",
    "        return self.pos, self.reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(obs_space, action_space):\n",
    "    # last_init=kernel_initializer= tf.keras.initializers.glorot_uniform()\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "    inputs = tf.keras.layers.Input(shape=(obs_space,))\n",
    "    out = tf.keras.layers.Dense(5, activation=tf.keras.activations.sigmoid)(inputs)\n",
    "    out = tf.keras.layers.Dense(5, activation=tf.keras.activations.sigmoid)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(action_space, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(obs, model, lower_bound=-1, upper_bound=1):\n",
    "    logits=model(obs)\n",
    "    sampled_actions = logits.numpy()\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    # legal_action = sampled_actions\n",
    "    return logits, legal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "    with tf.GradientTape() as tape:        \n",
    "        logits, action = policy(obs, model)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "        loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return next_obs, reward, done, grads, action, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(env, num_steps, model, loss_fn):\n",
    "    all_rewarsd=[]\n",
    "    all_grads=[]\n",
    "    all_obs=[]\n",
    "    all_action=[]\n",
    "    all_obs_prev=[]\n",
    "    all_losses=[]\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        all_obs_prev.append(np.copy(obs))\n",
    "        obs, reward, dode, grads, action, loss = play_one_step(env,obs,model,loss_fn)\n",
    "        all_rewarsd.append(np.copy(reward))\n",
    "        all_grads.append(np.copy(grads))\n",
    "        all_obs.append(np.copy(obs))\n",
    "        all_action.append(np.copy(action))\n",
    "        all_losses.append(np.copy(loss))\n",
    "    return all_rewarsd, all_grads, all_obs, all_action,all_obs_prev, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount):\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i]=np.mean(rewards[i])*pow(discount,i)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iteration_learn = 10\n",
    "n_max_steps = 5\n",
    "discount = 0.99\n",
    "learning_rate = 0.00001\n",
    "step_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=simple_env(step_size=step_size, targets=[100])\n",
    "first_pos=env.reset()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in model.trainable_variables:\n",
    "#   print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.layers[0].weights)\n",
    "# print(model.layers[1].weights)\n",
    "# print(model.layers[2].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env=simple_env()\n",
    "# model = get_actor(env.obsevation_space,env.action_space)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# obs = env.reset()\n",
    "# # obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "# obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "# with tf.GradientTape() as tape:        \n",
    "#     action = policy(obs,model)\n",
    "#     next_obs, reward, done = env.step(action)\n",
    "#     # print(obs)\n",
    "#     # print(next_obs)   \n",
    "#     # print(action)\n",
    "#     print(reward)\n",
    "#     logits = model(obs)\n",
    "#     y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "#     loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "# grads = tape.gradient(loss, model.trainable_variables)\n",
    "# # print(grads)\n",
    "# # for i in range(len(model.trainable_variables)):\n",
    "# #     rd=np.mean(reward)\n",
    "# #     grads[i]=tf.math.multiply(grads[i],rd)\n",
    "\n",
    "# optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "# print(\"*********************************************\")\n",
    "# print(grads)\n",
    "# print(\"*********************************************\")\n",
    "# for var in model.trainable_variables:\n",
    "#   print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n",
      "WARNING:tensorflow:Tensor._shape is private, use Tensor.shape instead. Tensor._shape will eventually be removed.\n"
     ]
    }
   ],
   "source": [
    "buffer=[]\n",
    "for iteration in range(n_iteration_learn):\n",
    "    all_rewards, all_grads, all_obs, all_action, all_obs_prev, all_losses = play_episodes(env,n_max_steps,model,loss_fn)\n",
    "    all_final_rewards = discount_rewards(all_rewards,discount)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        var = []\n",
    "        for step, final_reward in enumerate(all_final_rewards):            \n",
    "            for i in range(len(model.trainable_variables)):\n",
    "                all_grads[step][i]=tf.math.multiply(all_grads[step][i],final_reward)\n",
    "            var.append(all_grads[step][var_index])\n",
    "        all_mean_grads.append(tf.reduce_mean(var,axis=0))\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    episode_reward=tf.reduce_sum(all_final_rewards)\n",
    "    buffer.append([all_final_rewards,all_obs,all_action])\n",
    "    print(\"Episode * {} * Reward is ==> {}\".format(iteration, episode_reward))\n",
    "\n",
    "print(\"end\")\n",
    "\n",
    "# for var in model.trainable_variables:\n",
    "#   print(var, \"\\n\")\n",
    "\n",
    "\n",
    "print(env.pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
