{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_env():\n",
    "    def __init__(self, step_size=13,targets=[166,-88],max_steps=100):\n",
    "        self.pos=np.random.uniform(-10,10,len(targets))\n",
    "        self.max_steps=max_steps\n",
    "        self.step_size=step_size\n",
    "        self.targets=targets\n",
    "        self.max_num=1000\n",
    "        self.min_num=-1000\n",
    "        self.done=False\n",
    "        self.def_size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.counter_stop=0\n",
    "        self.obsevation_space=len(targets)\n",
    "        self.action_space=len(targets)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos=np.random.uniform(-10,10,len(self.targets))\n",
    "        self.size_pos=[abs(self.targets[i]-self.pos[i]) for i in range(len(self.targets))]\n",
    "        self.counter_stop=0\n",
    "        return self.pos\n",
    "\n",
    "    def step(self,action):\n",
    "        self.counter_stop+=1\n",
    "        if self.counter_stop>=self.max_steps:\n",
    "            self.done=True\n",
    "        self.last_pos=np.copy(self.pos) \n",
    "        self.last_size_pos=np.copy(self.size_pos)\n",
    "        self.reward=np.zeros(self.pos.shape)\n",
    "        for i in range(len(action)):\n",
    "            self.pos[i]=self.pos[i]+(action[i]*self.step_size)\n",
    "            self.size_pos[i]=abs(self.targets[i]-self.pos[i])\n",
    "            difference= self.last_size_pos[i]-self.size_pos[i]\n",
    "            self.reward[i]=difference/self.def_size_pos[i]\n",
    "        return self.pos, self.reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(obs_space, action_space):\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "    inputs = tf.keras.layers.Input(shape=(obs_space,))\n",
    "    out = tf.keras.layers.Dense(4, activation=\"relu\")(inputs)\n",
    "    outputs = tf.keras.layers.Dense(action_space, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(obs, model, lower_bound=-1, upper_bound=1):\n",
    "    sampled_actions=model(obs)\n",
    "    sampled_actions = sampled_actions.numpy()\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    return legal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "    with tf.GradientTape() as tape:        \n",
    "        obs=tf.expand_dims(tf.convert_to_tensor([12,-15]), 0)\n",
    "        action = policy(obs,model)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        logits = model(obs)\n",
    "        y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "        loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return next_obs, reward, done, grads, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(env, num_steps, model, loss_fn):\n",
    "    all_rewarsd=[]\n",
    "    all_grads=[]\n",
    "    all_obs=[]\n",
    "    all_action=[]\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "        all_rewarsd.append(np.copy(reward))\n",
    "        all_grads.append(np.copy(grads))\n",
    "        all_obs.append(np.copy(obs))\n",
    "        all_action.append(np.copy(action))\n",
    "    return all_rewarsd, all_grads, all_obs, all_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount):\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i]=np.mean(rewards[i])*pow(discount,i)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iteration_learn = 10\n",
    "n_max_steps = 3\n",
    "discount = 0.95\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=simple_env()\n",
    "first_pos=env.reset()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_123\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_124 (InputLayer)      [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_246 (Dense)           (None, 4)                 12        \n",
      "                                                                 \n",
      " dense_247 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_246/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.92634916,  0.4802642 , -0.09108782, -0.10725307],\n",
      "       [-0.5171292 , -0.42497873,  0.9786489 ,  0.75194454]],\n",
      "      dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_246/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_247/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[-0.03640034, -0.08363064],\n",
      "       [-0.00559258,  0.0086256 ],\n",
      "       [-0.00283928, -0.06190112],\n",
      "       [-0.09818655,  0.0180835 ]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_247/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for var in model.trainable_variables:\n",
    "  print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[<tf.Variable 'dense_246/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[-0.92634916,  0.4802642 , -0.09108782, -0.10725307],\n",
      "       [-0.5171292 , -0.42497873,  0.9786489 ,  0.75194454]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_246/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "[<tf.Variable 'dense_247/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[-0.03640034, -0.08363064],\n",
      "       [-0.00559258,  0.0086256 ],\n",
      "       [-0.00283928, -0.06190112],\n",
      "       [-0.09818655,  0.0180835 ]], dtype=float32)>, <tf.Variable 'dense_247/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weights)\n",
    "print(model.layers[1].weights)\n",
    "print(model.layers[2].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03866755 -0.02428906]\n",
      "*********************************************\n",
      "[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.09595227,  0.00214505, -0.0445703 , -0.02829772],\n",
      "       [-0.20836425,  0.00465807, -0.09678622, -0.06144964]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.03973014, -0.00088818,  0.01845485,  0.011717  ], dtype=float32)>, <tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
      "array([[-1.3789753 ,  0.73227215],\n",
      "       [-1.6629305 ,  0.8830598 ],\n",
      "       [-1.1900872 ,  0.6319676 ],\n",
      "       [-0.43286043,  0.22986028]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.35314867,  0.18753123], dtype=float32)>]\n",
      "*********************************************\n",
      "<tf.Variable 'dense_248/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.38746992, -0.2959872 , -0.09320949, -0.19253984],\n",
      "       [-0.90837944, -0.77615845, -0.58503896, -0.13044623]],\n",
      "      dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_248/bias:0' shape=(4,) dtype=float32, numpy=array([-0.0099992 ,  0.00996452, -0.00999829, -0.0099973 ], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_249/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[-0.05435757,  0.08066417],\n",
      "       [ 0.03472556,  0.03182571],\n",
      "       [-0.08641292, -0.09315006],\n",
      "       [-0.07010097, -0.0983613 ]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_249/bias:0' shape=(2,) dtype=float32, numpy=array([ 0.00999991, -0.00999983], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=simple_env()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "obs = env.reset()\n",
    "# obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "with tf.GradientTape() as tape:        \n",
    "    action = policy(obs,model)\n",
    "    next_obs, reward, done = env.step(action)\n",
    "    # print(obs)\n",
    "    # print(next_obs)   \n",
    "    # print(action)\n",
    "    print(reward)\n",
    "    logits = model(obs)\n",
    "    y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "    loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "# print(grads)\n",
    "# for i in range(len(model.trainable_variables)):\n",
    "#     rd=np.mean(reward)\n",
    "#     grads[i]=tf.math.multiply(grads[i],rd)\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "print(\"*********************************************\")\n",
    "print(grads)\n",
    "print(\"*********************************************\")\n",
    "for var in model.trainable_variables:\n",
    "  print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Reward is ==> -0.2601094804883344\n",
      "Episode * 1 * Reward is ==> -0.2294534170270457\n",
      "Episode * 2 * Reward is ==> -0.19903095967354714\n",
      "Episode * 3 * Reward is ==> -0.1697663796479697\n",
      "Episode * 4 * Reward is ==> -0.14255458775525834\n",
      "Episode * 5 * Reward is ==> -0.11782296894119003\n",
      "Episode * 6 * Reward is ==> -0.09565820072729901\n",
      "Episode * 7 * Reward is ==> -0.07595914336135792\n",
      "Episode * 8 * Reward is ==> -0.058527491309627526\n",
      "Episode * 9 * Reward is ==> -0.04312252607873244\n"
     ]
    }
   ],
   "source": [
    "buffer=[]\n",
    "for iteration in range(n_iteration_learn):\n",
    "    all_rewards, all_grads, all_obs, all_action = play_episodes(env,n_max_steps,model,loss_fn)\n",
    "    all_final_rewards = discount_rewards(all_rewards,discount)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        for step, final_reward in enumerate(all_final_rewards):\n",
    "            var = []\n",
    "            for i in range(len(model.trainable_variables)):\n",
    "                all_grads[step][i]=tf.math.multiply(all_grads[step][i],final_reward)\n",
    "            var.append(all_grads[step][var_index])\n",
    "        all_mean_grads.append(tf.reduce_mean(var,axis=0))\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    episode_reward=tf.reduce_sum(all_final_rewards)\n",
    "    buffer.append([all_final_rewards,all_obs,all_action])\n",
    "    print(\"Episode * {} * Reward is ==> {}\".format(iteration, episode_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
