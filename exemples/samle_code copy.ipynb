{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_env():\n",
    "    def __init__(self, step_size=13,targets=[166,-155],max_steps=100):\n",
    "        self.pos=np.random.uniform(-10,10,len(targets))\n",
    "        self.max_steps=max_steps\n",
    "        self.step_size=step_size\n",
    "        self.targets=targets\n",
    "        self.max_num=1000\n",
    "        self.min_num=-1000\n",
    "        self.done=False\n",
    "        self.def_size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.counter_stop=0\n",
    "        self.obsevation_space=len(targets)\n",
    "        self.action_space=len(targets)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos=np.random.uniform(-10,10,len(self.targets))\n",
    "        self.size_pos=[abs(self.targets[i]-self.pos[i]) for i in range(len(self.targets))]\n",
    "        self.counter_stop=0\n",
    "        return self.pos\n",
    "\n",
    "    def step(self,action):\n",
    "        self.counter_stop+=1\n",
    "        if self.counter_stop>=self.max_steps:\n",
    "            self.done=True\n",
    "        self.last_pos=np.copy(self.pos) \n",
    "        self.last_size_pos=np.copy(self.size_pos)\n",
    "        self.reward=np.zeros(self.pos.shape)\n",
    "        for i in range(len(action)):\n",
    "            self.pos[i]=self.pos[i]+(action[i]*self.step_size)\n",
    "            self.size_pos[i]=abs(self.targets[i]-self.pos[i])\n",
    "            difference= self.last_size_pos[i]-self.size_pos[i]\n",
    "            self.reward[i]=difference/self.def_size_pos[i]*100\n",
    "        return self.pos, self.reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(obs_space, action_space):\n",
    "    # last_init=kernel_initializer= tf.keras.initializers.glorot_uniform()\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "    inputs = tf.keras.layers.Input(shape=(obs_space,))\n",
    "    out = tf.keras.layers.Dense(10, activation=tf.keras.activations.relu)(inputs)\n",
    "    out = tf.keras.layers.Dense(10, activation=tf.keras.activations.relu)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(action_space, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(obs, model, lower_bound=-1, upper_bound=1):\n",
    "    logits=model(obs)\n",
    "    sampled_actions = logits.numpy()\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    # legal_action = sampled_actions\n",
    "    return logits, legal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "    with tf.GradientTape() as tape:        \n",
    "        logits, action = policy(obs, model)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "        loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return next_obs, reward, done, grads, action, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(env, num_steps, model, loss_fn):\n",
    "    all_rewarsd=[]\n",
    "    all_grads=[]\n",
    "    all_obs=[]\n",
    "    all_action=[]\n",
    "    all_obs_prev=[]\n",
    "    all_losses=[]\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        all_obs_prev.append(np.copy(obs))\n",
    "        obs, reward, dode, grads, action, loss = play_one_step(env,obs,model,loss_fn)\n",
    "        all_rewarsd.append(np.copy(reward))\n",
    "        all_grads.append(np.copy(grads))\n",
    "        all_obs.append(np.copy(obs))\n",
    "        all_action.append(np.copy(action))\n",
    "        all_losses.append(np.copy(loss))\n",
    "    return all_rewarsd, all_grads, all_obs, all_action,all_obs_prev, all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount):\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i]=np.mean(rewards[i])*pow(discount,i)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iteration_learn = 100\n",
    "n_max_steps = 10\n",
    "discount = 1\n",
    "learning_rate = 0.01\n",
    "step_size=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=simple_env(step_size=step_size, targets=[100])\n",
    "first_pos=env.reset()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var in model.trainable_variables:\n",
    "#   print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.layers[0].weights)\n",
    "# print(model.layers[1].weights)\n",
    "# print(model.layers[2].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env=simple_env()\n",
    "# model = get_actor(env.obsevation_space,env.action_space)\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "# obs = env.reset()\n",
    "# # obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "# obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "# with tf.GradientTape() as tape:        \n",
    "#     action = policy(obs,model)\n",
    "#     next_obs, reward, done = env.step(action)\n",
    "#     # print(obs)\n",
    "#     # print(next_obs)   \n",
    "#     # print(action)\n",
    "#     print(reward)\n",
    "#     logits = model(obs)\n",
    "#     y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "#     loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "# grads = tape.gradient(loss, model.trainable_variables)\n",
    "# # print(grads)\n",
    "# # for i in range(len(model.trainable_variables)):\n",
    "# #     rd=np.mean(reward)\n",
    "# #     grads[i]=tf.math.multiply(grads[i],rd)\n",
    "\n",
    "# optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "# print(\"*********************************************\")\n",
    "# print(grads)\n",
    "# print(\"*********************************************\")\n",
    "# for var in model.trainable_variables:\n",
    "#   print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Reward is ==> -4.6534493136466635\n",
      "Episode * 1 * Reward is ==> -37.36801220517566\n",
      "Episode * 2 * Reward is ==> -110.8355455562026\n",
      "Episode * 3 * Reward is ==> -138.69153146308034\n",
      "Episode * 4 * Reward is ==> -140.95209339093182\n",
      "Episode * 5 * Reward is ==> -140.43926983078882\n",
      "Episode * 6 * Reward is ==> -141.51992394645646\n",
      "Episode * 7 * Reward is ==> -136.83980335088208\n",
      "Episode * 8 * Reward is ==> -141.43661033302934\n",
      "Episode * 9 * Reward is ==> -141.38496007702906\n",
      "Episode * 10 * Reward is ==> -141.52843432265132\n",
      "Episode * 11 * Reward is ==> -141.4972568867836\n",
      "Episode * 12 * Reward is ==> -141.5012218717277\n",
      "Episode * 13 * Reward is ==> -138.23020377760898\n",
      "Episode * 14 * Reward is ==> -141.39804452734452\n",
      "Episode * 15 * Reward is ==> -141.5282065469205\n",
      "Episode * 16 * Reward is ==> -141.5192178416909\n",
      "Episode * 17 * Reward is ==> -141.52913367850635\n",
      "Episode * 18 * Reward is ==> -141.52980941317452\n",
      "Episode * 19 * Reward is ==> -141.53060831546003\n",
      "Episode * 20 * Reward is ==> -141.53074666812617\n",
      "Episode * 21 * Reward is ==> -141.09192997377136\n",
      "Episode * 22 * Reward is ==> -141.5307483553538\n",
      "Episode * 23 * Reward is ==> -141.372846626218\n",
      "Episode * 24 * Reward is ==> -141.50657375778837\n",
      "Episode * 25 * Reward is ==> -140.80613723345732\n",
      "Episode * 26 * Reward is ==> -141.53331041051874\n",
      "Episode * 27 * Reward is ==> -141.3251816018934\n",
      "Episode * 28 * Reward is ==> -141.53447712842888\n",
      "Episode * 29 * Reward is ==> -141.51301306206034\n",
      "Episode * 30 * Reward is ==> -141.48873807445057\n",
      "Episode * 31 * Reward is ==> -141.50330053617495\n",
      "Episode * 32 * Reward is ==> -141.38133675568122\n",
      "Episode * 33 * Reward is ==> -141.53487278330945\n",
      "Episode * 34 * Reward is ==> -141.5349124331589\n",
      "Episode * 35 * Reward is ==> -141.53384441806546\n",
      "Episode * 36 * Reward is ==> -141.20290484067328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [185], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     all_mean_grads\u001b[39m.\u001b[39mappend(tf\u001b[39m.\u001b[39mreduce_mean(var,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m grad \u001b[39min\u001b[39;00m all_grads:\n\u001b[1;32m---> 18\u001b[0m     optimizer\u001b[39m.\u001b[39;49mapply_gradients(\u001b[39mzip\u001b[39;49m(grad, model\u001b[39m.\u001b[39;49mtrainable_variables))\n\u001b[0;32m     19\u001b[0m \u001b[39m# optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\u001b[39;00m\n\u001b[0;32m     20\u001b[0m episode_reward\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mreduce_sum(all_final_rewards)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:730\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    712\u001b[0m     \u001b[39mnot\u001b[39;00m experimental_aggregate_gradients\n\u001b[0;32m    713\u001b[0m     \u001b[39mand\u001b[39;00m strategy\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m     )\n\u001b[0;32m    723\u001b[0m ):\n\u001b[0;32m    724\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    725\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`experimental_aggregate_gradients=False is not supported \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    726\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor ParameterServerStrategy and CentralStorageStrategy. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    727\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsed: strategy=\u001b[39m\u001b[39m{\u001b[39;00mstrategy\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    728\u001b[0m     )\n\u001b[1;32m--> 730\u001b[0m apply_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare(var_list)\n\u001b[0;32m    731\u001b[0m \u001b[39mif\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m    732\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_unaggregated_gradients(\n\u001b[0;32m    733\u001b[0m         grads_and_vars\n\u001b[0;32m    734\u001b[0m     )\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py:1097\u001b[0m, in \u001b[0;36mOptimizerV2._prepare\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     apply_state[(var_device, var_dtype)] \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1096\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(var_device):\n\u001b[1;32m-> 1097\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_local(var_device, var_dtype, apply_state)\n\u001b[0;32m   1099\u001b[0m \u001b[39mreturn\u001b[39;00m apply_state\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:139\u001b[0m, in \u001b[0;36mAdam._prepare_local\u001b[1;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[0;32m    137\u001b[0m beta_1_t \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39midentity(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_hyper(\u001b[39m\"\u001b[39m\u001b[39mbeta_1\u001b[39m\u001b[39m\"\u001b[39m, var_dtype))\n\u001b[0;32m    138\u001b[0m beta_2_t \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39midentity(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_hyper(\u001b[39m\"\u001b[39m\u001b[39mbeta_2\u001b[39m\u001b[39m\"\u001b[39m, var_dtype))\n\u001b[1;32m--> 139\u001b[0m beta_1_power \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mpow(beta_1_t, local_step)\n\u001b[0;32m    140\u001b[0m beta_2_power \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mpow(beta_2_t, local_step)\n\u001b[0;32m    141\u001b[0m lr \u001b[39m=\u001b[39m apply_state[(var_device, var_dtype)][\u001b[39m\"\u001b[39m\u001b[39mlr_t\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m (\n\u001b[0;32m    142\u001b[0m     tf\u001b[39m.\u001b[39msqrt(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta_2_power) \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta_1_power)\n\u001b[0;32m    143\u001b[0m )\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:694\u001b[0m, in \u001b[0;36mpow\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the power of one value to another.\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \n\u001b[0;32m    674\u001b[0m \u001b[39mGiven a tensor `x` and a tensor `y`, this operation computes \\\\(x^y\\\\) for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39m  A `Tensor`.\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(name, \u001b[39m\"\u001b[39m\u001b[39mPow\u001b[39m\u001b[39m\"\u001b[39m, [x]) \u001b[39mas\u001b[39;00m name:\n\u001b[1;32m--> 694\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49m_pow(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32me:\\work\\ENV\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:7094\u001b[0m, in \u001b[0;36m_pow\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   7092\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   7093\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 7094\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   7095\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mPow\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y)\n\u001b[0;32m   7096\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   7097\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "buffer=[]\n",
    "for iteration in range(n_iteration_learn):\n",
    "    all_rewards, all_grads, all_obs, all_action, all_obs_prev, all_losses = play_episodes(env,n_max_steps,model,loss_fn)\n",
    "    all_final_rewards = discount_rewards(all_rewards,discount)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        var = []\n",
    "        for step, final_reward in enumerate(all_final_rewards):            \n",
    "            for i in range(len(model.trainable_variables)):\n",
    "                all_grads[step][i]=tf.math.multiply(all_grads[step][i],final_reward)\n",
    "            var.append(all_grads[step][var_index])\n",
    "        all_mean_grads.append(tf.reduce_mean(var,axis=0))\n",
    "    for grad in all_grads:\n",
    "        optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "    # optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    episode_reward=tf.reduce_sum(all_final_rewards)\n",
    "    buffer.append([all_final_rewards,all_obs,all_action])\n",
    "    print(\"Episode * {} * Reward is ==> {}\".format(iteration, episode_reward))\n",
    "\n",
    "print(\"end\")\n",
    "\n",
    "# for var in model.trainable_variables:\n",
    "#   print(var, \"\\n\")\n",
    "\n",
    "\n",
    "print(env.pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
