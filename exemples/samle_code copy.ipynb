{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_env():\n",
    "    def __init__(self, step_size=13,targets=[166,-88],max_steps=100):\n",
    "        self.pos=np.random.uniform(-10,10,len(targets))\n",
    "        self.max_steps=max_steps\n",
    "        self.step_size=step_size\n",
    "        self.targets=targets\n",
    "        self.max_num=1000\n",
    "        self.min_num=-1000\n",
    "        self.done=False\n",
    "        self.def_size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.counter_stop=0\n",
    "        self.obsevation_space=len(targets)\n",
    "        self.action_space=len(targets)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos=np.random.uniform(-10,10,len(self.targets))\n",
    "        self.size_pos=[abs(self.targets[i]-self.pos[i]) for i in range(len(self.targets))]\n",
    "        self.counter_stop=0\n",
    "        return self.pos\n",
    "\n",
    "    def step(self,action):\n",
    "        self.counter_stop+=1\n",
    "        if self.counter_stop>=self.max_steps:\n",
    "            self.done=True\n",
    "        self.last_pos=np.copy(self.pos) \n",
    "        self.last_size_pos=np.copy(self.size_pos)\n",
    "        self.reward=np.zeros(self.pos.shape)\n",
    "        for i in range(len(action)):\n",
    "            self.pos[i]=self.pos[i]+(action[i]*self.step_size)\n",
    "            self.size_pos[i]=abs(self.targets[i]-self.pos[i])\n",
    "            difference= self.last_size_pos[i]-self.size_pos[i]\n",
    "            self.reward[i]=difference/self.def_size_pos[i]\n",
    "        return self.pos, self.reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(obs_space, action_space):\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.1, maxval=0.1)\n",
    "    inputs = tf.keras.layers.Input(shape=(obs_space,))\n",
    "    out = tf.keras.layers.Dense(4, activation=\"relu\")(inputs)\n",
    "    outputs = tf.keras.layers.Dense(action_space, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(obs, model, lower_bound=-1, upper_bound=1):\n",
    "    sampled_actions=model(obs)\n",
    "    sampled_actions = sampled_actions.numpy()\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    return legal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "    with tf.GradientTape() as tape:        \n",
    "        obs=tf.expand_dims(tf.convert_to_tensor([12,-15]), 0)\n",
    "        action = policy(obs,model)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        logits = model(obs)\n",
    "        y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "        loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return next_obs, reward, done, grads, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(env, num_steps, model, loss_fn):\n",
    "    all_rewarsd=[]\n",
    "    all_grads=[]\n",
    "    all_obs=[]\n",
    "    all_action=[]\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "        all_rewarsd.append(np.copy(reward))\n",
    "        all_grads.append(np.copy(grads))\n",
    "        all_obs.append(np.copy(obs))\n",
    "        all_action.append(np.copy(action))\n",
    "    return all_rewarsd, all_grads, all_obs, all_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount):\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i]=np.mean(rewards[i])*pow(discount,i)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iteration_learn = 10\n",
    "n_max_steps = 3\n",
    "discount = 0.95\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=simple_env()\n",
    "first_pos=env.reset()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_121\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_122 (InputLayer)      [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_242 (Dense)           (None, 4)                 12        \n",
      "                                                                 \n",
      " dense_243 (Dense)           (None, 2)                 10        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22\n",
      "Trainable params: 22\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_242/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.16038346, -0.9170389 ,  0.6178055 , -0.67084146],\n",
      "       [ 0.5983014 , -0.20601225, -0.77527   ,  0.92489314]],\n",
      "      dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_242/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_243/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[-0.01579966, -0.03933489],\n",
      "       [-0.01232827,  0.04514391],\n",
      "       [-0.00623178,  0.08163605],\n",
      "       [ 0.03753508,  0.09060925]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_243/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for var in model.trainable_variables:\n",
    "  print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[<tf.Variable 'dense_242/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.16038346, -0.9170389 ,  0.6178055 , -0.67084146],\n",
      "       [ 0.5983014 , -0.20601225, -0.77527   ,  0.92489314]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_242/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]\n",
      "[<tf.Variable 'dense_243/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[-0.01579966, -0.03933489],\n",
      "       [-0.01232827,  0.04514391],\n",
      "       [-0.00623178,  0.08163605],\n",
      "       [ 0.03753508,  0.09060925]], dtype=float32)>, <tf.Variable 'dense_243/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weights)\n",
    "print(model.layers[1].weights)\n",
    "print(model.layers[2].weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00375837 0.05316206]\n",
      "*********************************************\n",
      "[<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
      "array([[-0.00054769, -0.00082595,  0.        ,  0.        ],\n",
      "       [ 0.00047007,  0.00070889, -0.        , -0.        ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4,), dtype=float32, numpy=\n",
      "array([ 9.9502504e-05,  1.5005590e-04, -0.0000000e+00, -0.0000000e+00],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
      "array([[-1.0181728e-04, -2.9923315e-03],\n",
      "       [-9.3562187e-05, -2.7497204e-03],\n",
      "       [-0.0000000e+00, -0.0000000e+00],\n",
      "       [-0.0000000e+00, -0.0000000e+00]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.00010286, -0.00302301], dtype=float32)>]\n",
      "*********************************************\n",
      "<tf.Variable 'dense_244/kernel:0' shape=(2, 4) dtype=float32, numpy=\n",
      "array([[ 0.47558242,  0.58713174,  0.87569547, -0.8393936 ],\n",
      "       [ 0.74212915,  0.8550659 , -0.08607602, -0.9991238 ]],\n",
      "      dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_244/bias:0' shape=(4,) dtype=float32, numpy=array([-0.00969198, -0.00979361,  0.        ,  0.        ], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_245/kernel:0' shape=(4, 2) dtype=float32, numpy=\n",
      "array([[ 0.10317035, -0.02610608],\n",
      "       [-0.06472705, -0.03711787],\n",
      "       [ 0.03297565,  0.07934368],\n",
      "       [ 0.04652386,  0.01749551]], dtype=float32)> \n",
      "\n",
      "<tf.Variable 'dense_245/bias:0' shape=(2,) dtype=float32, numpy=array([0.00970174, 0.00998955], dtype=float32)> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=simple_env()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "obs = env.reset()\n",
    "# obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "with tf.GradientTape() as tape:        \n",
    "    action = policy(obs,model)\n",
    "    next_obs, reward, done = env.step(action)\n",
    "    # print(obs)\n",
    "    # print(next_obs)   \n",
    "    # print(action)\n",
    "    print(reward)\n",
    "    logits = model(obs)\n",
    "    y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "    loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "grads = tape.gradient(loss, model.trainable_variables)\n",
    "# print(grads)\n",
    "for i in range(len(model.trainable_variables)):\n",
    "    rd=np.mean(reward)\n",
    "    grads[i]=tf.math.multiply(grads[i],rd)\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "print(\"*********************************************\")\n",
    "print(grads)\n",
    "print(\"*********************************************\")\n",
    "for var in model.trainable_variables:\n",
    "  print(var, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Reward is ==> -0.12075142097020947\n",
      "Episode * 1 * Reward is ==> -0.12312596904843134\n",
      "Episode * 2 * Reward is ==> -0.12464709266497748\n",
      "Episode * 3 * Reward is ==> -0.1253388210654547\n",
      "Episode * 4 * Reward is ==> -0.12551554452853386\n",
      "Episode * 5 * Reward is ==> -0.12546609742130424\n",
      "Episode * 6 * Reward is ==> -0.12536486314890688\n",
      "Episode * 7 * Reward is ==> -0.12528376702188398\n",
      "Episode * 8 * Reward is ==> -0.1252372348654574\n",
      "Episode * 9 * Reward is ==> -0.125218069090006\n"
     ]
    }
   ],
   "source": [
    "buffer=[]\n",
    "for iteration in range(n_iteration_learn):\n",
    "    all_rewards, all_grads, all_obs, all_action = play_episodes(env,n_max_steps,model,loss_fn)\n",
    "    all_final_rewards = discount_rewards(all_rewards,discount)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        for step, final_reward in enumerate(all_final_rewards):\n",
    "            var = []\n",
    "            for i in range(len(model.trainable_variables)):\n",
    "                all_grads[step][i]=tf.math.multiply(all_grads[step][i],final_reward)\n",
    "            var.append(all_grads[step][var_index])\n",
    "        all_mean_grads.append(tf.reduce_mean(var,axis=0))\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    episode_reward=tf.reduce_sum(all_final_rewards)\n",
    "    buffer.append([all_final_rewards,all_obs,all_action])\n",
    "    print(\"Episode * {} * Reward is ==> {}\".format(iteration, episode_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
