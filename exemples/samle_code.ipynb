{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_env():\n",
    "    def __init__(self, step_size=13,targets=[166,-88],max_steps=100):\n",
    "        self.pos=np.random.uniform(-10,10,len(targets))\n",
    "        self.max_steps=max_steps\n",
    "        self.step_size=step_size\n",
    "        self.targets=targets\n",
    "        self.max_num=1000\n",
    "        self.min_num=-1000\n",
    "        self.done=False\n",
    "        self.def_size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.size_pos=[abs(targets[i]-self.pos[i]) for i in range(len(targets))]\n",
    "        self.counter_stop=0\n",
    "        self.obsevation_space=len(targets)\n",
    "        self.action_space=len(targets)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.pos=np.random.uniform(-10,10,len(self.targets))\n",
    "        self.size_pos=[abs(self.targets[i]-self.pos[i]) for i in range(len(self.targets))]\n",
    "        self.counter_stop=0\n",
    "        return self.pos\n",
    "\n",
    "    def step(self,action):\n",
    "        self.counter_stop+=1\n",
    "        if self.counter_stop>=self.max_steps:\n",
    "            self.done=True\n",
    "        self.last_pos=np.copy(self.pos) \n",
    "        self.last_size_pos=np.copy(self.size_pos)\n",
    "        self.reward=np.zeros(self.pos.shape)\n",
    "        for i in range(len(action)):\n",
    "            self.pos[i]=self.pos[i]+(action[i]*self.step_size)\n",
    "            self.size_pos[i]=abs(self.targets[i]-self.pos[i])\n",
    "            difference= self.last_size_pos[i]-self.size_pos[i]\n",
    "            self.reward[i]=difference/self.def_size_pos[i]\n",
    "        return self.pos, self.reward, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(obs_space, action_space):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.01, maxval=0.01)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(obs_space,))\n",
    "    out = tf.keras.layers.Dense(10, activation=\"relu\")(inputs)\n",
    "    out = tf.keras.layers.Dense(10, activation=\"relu\")(out)\n",
    "    outputs = tf.keras.layers.Dense(action_space, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # # Our upper bound is 2.0 for Pendulum.\n",
    "    # outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(obs, model, lower_bound=-1, upper_bound=1):\n",
    "    sampled_actions=model(obs)\n",
    "    sampled_actions = sampled_actions.numpy()\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "    return legal_action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env=simple_env()\n",
    "# model = get_actor(env.obsevation_space,env.action_space)\n",
    "# print(env.pos)\n",
    "# action = policy([25,-112],model)\n",
    "# print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    obs=tf.expand_dims(tf.convert_to_tensor(obs), 0)\n",
    "    with tf.GradientTape() as tape:        \n",
    "        action = policy(obs,model)\n",
    "        next_obs, reward, done = env.step(action)\n",
    "        logits = model(obs)\n",
    "        y_target=tf.reduce_mean(tf.Variable(reward))\n",
    "        loss= tf.reduce_mean(loss_fn(y_target,logits))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    return next_obs, reward, done, grads, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env=simple_env()\n",
    "# first_pos=env.reset()\n",
    "# model = get_actor(env.obsevation_space,env.action_space)\n",
    "# loss_fn=tf.keras.losses.MeanSquaredError()\n",
    "# print(play_one_step(env,first_pos,model,loss_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(env, num_steps, model, loss_fn):\n",
    "    all_rewarsd=[]\n",
    "    all_grads=[]\n",
    "    all_obs=[]\n",
    "    all_action=[]\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        obs, reward, dode, grads, action= play_one_step(env,obs,model,loss_fn)\n",
    "        all_rewarsd.append(np.copy(reward))\n",
    "        all_grads.append(np.copy(grads))\n",
    "        all_obs.append(np.copy(obs))\n",
    "        all_action.append(np.copy(action))\n",
    "    return all_rewarsd, all_grads, all_obs, all_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount):\n",
    "    for i in range(len(rewards)):\n",
    "        rewards[i]=np.mean(rewards[i])*pow(discount,i)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount_rewards([[2,4],-4,12],0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iteration_learn = 10\n",
    "n_max_steps = 5\n",
    "discount = 0.95\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=simple_env()\n",
    "first_pos=env.reset()\n",
    "model = get_actor(env.obsevation_space,env.action_space)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Reward is ==> -0.005216534825255978\n",
      "Episode * 1 * Reward is ==> 0.0005662935428870063\n",
      "Episode * 2 * Reward is ==> -0.012756238379365221\n",
      "Episode * 3 * Reward is ==> -0.02867735687740393\n",
      "Episode * 4 * Reward is ==> -0.012746746300070213\n",
      "Episode * 5 * Reward is ==> -0.21464134819012698\n",
      "Episode * 6 * Reward is ==> -0.5070200133451533\n",
      "Episode * 7 * Reward is ==> -0.5070200133451533\n",
      "Episode * 8 * Reward is ==> -0.5070200133451536\n",
      "Episode * 9 * Reward is ==> -0.5070200133451535\n"
     ]
    }
   ],
   "source": [
    "buffer=[]\n",
    "for iteration in range(n_iteration_learn):\n",
    "    all_rewards, all_grads, all_obs, all_action = play_episodes(env,n_max_steps,model,loss_fn)\n",
    "    all_final_rewards = discount_rewards(all_rewards,discount)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        for step, final_reward in enumerate(all_final_rewards):\n",
    "            var = []\n",
    "            for i in range(len(model.trainable_variables)):\n",
    "                all_grads[step][i]=tf.math.multiply(all_grads[step][i],final_reward)\n",
    "            var.append(all_grads[step][var_index])\n",
    "        all_mean_grads.append(tf.reduce_mean(var,axis=0))\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    episode_reward=tf.reduce_sum(all_final_rewards)\n",
    "    buffer.append([all_final_rewards,all_obs,all_action])\n",
    "    print(\"Episode * {} * Reward is ==> {}\".format(iteration, episode_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m np\u001b[39m.\u001b[39;49mmatmul( [[\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m],[\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m]], \u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "np.matmul( [[1, 2, 3, 4],[1, 2, 3, 4]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(42)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    total_rewards = sum(map(sum, all_rewards))                     # Not shown in the book\n",
    "    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          # Not shown\n",
    "        iteration, total_rewards / n_episodes_per_update), end=\"\") # Not shown\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                       discount_rate)\n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "             for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                 for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e010a72b03347896d5a708620cb2a6934ef5db96cb99d8f5b4211fde9732cbc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
